{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# TAHAP 5\n",
        "## Penalaran Komputer UAS\n",
        "## **Anggota:**\n",
        "## Haidar Dimas Heryanto - 202210370311088\n",
        "## Zeedan Mustami Argani - 202210370311104"
      ],
      "metadata": {
        "id": "crS2NMF7eROv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ss0ki1TRdoFV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "# --- Konfigurasi Path ---\n",
        "# Pastikan path ini sesuai dengan struktur folder Anda di Google Drive\n",
        "BASE_DRIVE_PATH = \"/content/drive/MyDrive/Penalaran Komputer UAS/\"\n",
        "PATH_PROCESSED = os.path.join(BASE_DRIVE_PATH, \"data/processed\")\n",
        "PROCESSED_CSV_FILENAME = \"cases_processed.csv\"\n",
        "processed_data_filepath = os.path.join(PATH_PROCESSED, PROCESSED_CSV_FILENAME)\n",
        "\n",
        "# --- Fungsi Ekstraksi dan Kategorisasi ---\n",
        "def get_kategori_amar(full_text):\n",
        "    \"\"\"\n",
        "    Mengekstrak dan mengkategorikan amar putusan dari teks lengkap sebuah dokumen.\n",
        "    \"\"\"\n",
        "    if pd.isna(full_text) or not isinstance(full_text, str):\n",
        "        return \"Teks Tidak Valid\"\n",
        "\n",
        "    match = re.search(\n",
        "        r\"MENGADILI,?\\s*(.*?)(Demikianlah diputuskan|Memperhatikan pasal-pasal)\",\n",
        "        full_text,\n",
        "        re.IGNORECASE | re.DOTALL\n",
        "    )\n",
        "\n",
        "    if not match:\n",
        "        return \"Amar Tidak Ditemukan\"\n",
        "\n",
        "    amar_text = match.group(1).lower()\n",
        "\n",
        "    if 'menolak permohonan' in amar_text:\n",
        "        return \"Menolak permohonan\"\n",
        "    elif 'mengabulkan seluruhnya' in amar_text or 'mengabulkan permohonan banding' in amar_text:\n",
        "        return \"Mengabulkan seluruhnya\"\n",
        "    elif 'mengabulkan sebagian' in amar_text:\n",
        "        return \"Mengabulkan sebagian\"\n",
        "    elif 'tidak dapat diterima' in amar_text:\n",
        "        return \"Tidak dapat diterima\"\n",
        "    else:\n",
        "        return \"Lain-lain (Perlu Cek Manual)\"\n",
        "\n",
        "# --- Proses Utama: Baca, Proses, dan Simpan ---\n",
        "try:\n",
        "    # 1. Membaca file CSV yang ada\n",
        "    print(f\"Membaca file dari: {processed_data_filepath}...\")\n",
        "    df_cases = pd.read_csv(processed_data_filepath)\n",
        "    print(\"File berhasil dimuat.\")\n",
        "\n",
        "    # 2. Membuat kolom baru 'amar_kategori'\n",
        "    print(\"Memproses dan membuat kolom 'amar_kategori'...\")\n",
        "    df_cases['amar_kategori'] = df_cases['text_full'].apply(get_kategori_amar)\n",
        "    print(\"Kolom baru berhasil dibuat.\")\n",
        "\n",
        "    # Tampilkan beberapa baris untuk verifikasi sebelum menyimpan\n",
        "    print(\"\\nVerifikasi hasil (5 baris pertama):\")\n",
        "    display(df_cases[['case_id', 'amar_putusan', 'amar_kategori']].head())\n",
        "\n",
        "    # 3. Menyimpan kembali DataFrame yang sudah diupdate ke file yang sama\n",
        "    print(f\"\\nMenyimpan kembali DataFrame yang diperbarui ke {processed_data_filepath}...\")\n",
        "    df_cases.to_csv(processed_data_filepath, index=False, encoding='utf-8')\n",
        "    print(\"✅ Berhasil! File `cases_processed.csv` telah diperbarui dengan kolom 'amar_kategori'.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"❌ Gagal: File tidak ditemukan di {processed_data_filepath}.\")\n",
        "    print(\"Pastikan file `cases_processed.csv` sudah ada dari hasil Tahap 2.\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Terjadi error: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 05_Model_Evaluation.ipynb\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "uRMD7RW2dvh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configuration Section ---\n",
        "# !!! IMPORTANT: Ensure these paths match your Google Drive structure\n",
        "# and the outputs from previous notebooks !!!\n",
        "BASE_DRIVE_PATH = \"/content/drive/MyDrive/Penalaran Komputer UAS/\" # Change to your project folder\n",
        "\n",
        "# Paths for input data\n",
        "PATH_EVAL_INPUT = os.path.join(BASE_DRIVE_PATH, \"data/eval\")\n",
        "QUERIES_JSON_FILENAME = \"queries.json\" # Contains ground_truth_ids for retrieval\n",
        "\n",
        "PATH_RESULTS_INPUT = os.path.join(BASE_DRIVE_PATH, \"data/results\")\n",
        "PREDICTIONS_CSV_FILENAME = \"predictions.csv\" # Contains retrieved top_k_ids and predicted_solutions\n",
        "\n",
        "PATH_PROCESSED_INPUT = os.path.join(BASE_DRIVE_PATH, \"data/processed\") # For actual case data if needed\n",
        "PROCESSED_CSV_FILENAME = \"cases_processed.csv\"\n",
        "\n",
        "\n",
        "# Path for output metrics\n",
        "PATH_EVAL_OUTPUT = os.path.join(BASE_DRIVE_PATH, \"data/eval\") # Outputting metrics here too\n",
        "RETRIEVAL_METRICS_FILENAME = \"retrieval_metrics.csv\"\n",
        "PREDICTION_METRICS_FILENAME = \"prediction_metrics.csv\"\n",
        "\n",
        "# K value for retrieval evaluation (e.g., evaluate Precision@K, Recall@K)\n",
        "K_EVAL = 5 # Should match the 'k' used for generating top_k_case_ids in predictions.csv"
      ],
      "metadata": {
        "id": "oqmIQK9xdxYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Helper Functions ---\n",
        "\n",
        "def load_data_for_evaluation():\n",
        "    \"\"\"Loads queries, predictions, and processed cases.\"\"\"\n",
        "    global df_queries, df_predictions, df_cases\n",
        "    queries_json_filepath = os.path.join(PATH_EVAL_INPUT, QUERIES_JSON_FILENAME)\n",
        "    predictions_csv_filepath = os.path.join(PATH_RESULTS_INPUT, PREDICTIONS_CSV_FILENAME)\n",
        "    cases_csv_filepath = os.path.join(PATH_PROCESSED_INPUT, PROCESSED_CSV_FILENAME)\n",
        "\n",
        "    try:\n",
        "        with open(queries_json_filepath, 'r', encoding='utf-8') as f:\n",
        "            queries_data_list = json.load(f)\n",
        "        df_queries = pd.DataFrame(queries_data_list)\n",
        "        # Ensure ground_truth_ids is a list, even if empty\n",
        "        df_queries['ground_truth_ids'] = df_queries['ground_truth_ids'].apply(lambda x: x if isinstance(x, list) else [])\n",
        "        print(f\"Loaded queries from: {queries_json_filepath} - {len(df_queries)} queries.\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Queries file not found at {queries_json_filepath}.\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading queries.json: {e}\")\n",
        "        return False\n",
        "\n",
        "    try:\n",
        "        df_predictions = pd.read_csv(predictions_csv_filepath)\n",
        "        # Convert 'top_5_case_ids' string back to list\n",
        "        df_predictions['top_k_case_ids_list'] = df_predictions['top_5_case_ids'].apply(\n",
        "            lambda x: [case_id.strip() for case_id in str(x).split(',')] if pd.notna(x) and str(x).strip() else []\n",
        "        )\n",
        "        print(f\"Loaded predictions from: {predictions_csv_filepath} - {len(df_predictions)} predictions.\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Predictions file not found at {predictions_csv_filepath}.\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading predictions.csv: {e}\")\n",
        "        return False\n",
        "\n",
        "    try:\n",
        "        df_cases = pd.read_csv(cases_csv_filepath)\n",
        "        print(f\"Loaded processed cases from: {cases_csv_filepath} - {len(df_cases)} cases.\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Warning: Processed cases file not found at {cases_csv_filepath}. Prediction evaluation against actuals might be limited.\")\n",
        "        df_cases = None # Set to None if not found\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {cases_csv_filepath}: {e}\")\n",
        "        df_cases = None\n",
        "\n",
        "    return True\n",
        "\n",
        "\n",
        "def calculate_retrieval_metrics_for_query(retrieved_ids, ground_truth_ids):\n",
        "    \"\"\"\n",
        "    Calculates Precision@k, Recall@k, F1-score@k for a single query.\n",
        "    k is implicitly len(retrieved_ids).\n",
        "    \"\"\"\n",
        "    if not ground_truth_ids: # No ground truth, cannot evaluate\n",
        "        return 0, 0, 0\n",
        "\n",
        "    # Ensure retrieved_ids is a list of strings, handle empty strings or NaNs\n",
        "    # This is important if top_k_case_ids_list can contain empty strings from parsing\n",
        "    valid_retrieved_ids = [rid for rid in retrieved_ids if rid and pd.notna(rid)]\n",
        "\n",
        "    if not valid_retrieved_ids: # No valid retrieved items\n",
        "        return 0, 0, 0 # Precision is 0, Recall is 0 if ground truth exists but nothing retrieved\n",
        "\n",
        "    # True positives: intersection of retrieved and ground truth\n",
        "    tp_set = set(valid_retrieved_ids) & set(ground_truth_ids)\n",
        "    tp = len(tp_set)\n",
        "\n",
        "    precision_at_k = tp / len(valid_retrieved_ids) if len(valid_retrieved_ids) > 0 else 0\n",
        "    recall_at_k = tp / len(ground_truth_ids) if len(ground_truth_ids) > 0 else 0\n",
        "    f1_at_k = (2 * precision_at_k * recall_at_k) / (precision_at_k + recall_at_k) if (precision_at_k + recall_at_k) > 0 else 0\n",
        "\n",
        "    return precision_at_k, recall_at_k, f1_at_k"
      ],
      "metadata": {
        "id": "YXM2hR6keFEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Tahap 5.a: Evaluasi Retrieval ---\n",
        "def evaluate_retrieval_performance():\n",
        "    global df_queries, df_predictions\n",
        "    print(\"\\n--- Evaluating Retrieval Performance ---\")\n",
        "\n",
        "    # Merge predictions with ground truth queries\n",
        "    # Assuming query_id is the common key\n",
        "    # df_predictions might have multiple rows per query_id if different algorithms were logged.\n",
        "    eval_df = pd.merge(df_predictions, df_queries[['query_id', 'ground_truth_ids']], on='query_id', how='left')\n",
        "\n",
        "    if 'ground_truth_ids' not in eval_df.columns or eval_df['ground_truth_ids'].isnull().all():\n",
        "        print(\"Warning: 'ground_truth_ids' not found or all null in merged data. Cannot perform retrieval evaluation.\")\n",
        "        return pd.DataFrame() # Return empty dataframe\n",
        "\n",
        "    retrieval_metrics_log = []\n",
        "    for index, row in eval_df.iterrows():\n",
        "        query_id = row['query_id']\n",
        "        retrieved_ids = row['top_k_case_ids_list'][:K_EVAL] # Evaluate at K_EVAL\n",
        "        ground_truth = row['ground_truth_ids']\n",
        "        retrieval_method = row.get('retrieval_method', 'UNKNOWN') # Get method if logged\n",
        "        prediction_algorithm = row.get('prediction_algorithm', 'UNKNOWN') # Get algo if logged\n",
        "\n",
        "        if not isinstance(ground_truth, list) or not ground_truth: # Should be caught by apply earlier, but double check\n",
        "             print(f\"Warning: No ground truth for query_id {query_id}. Skipping retrieval eval for this entry.\")\n",
        "             precision, recall, f1 = 0,0,0\n",
        "        else:\n",
        "            precision, recall, f1 = calculate_retrieval_metrics_for_query(retrieved_ids, ground_truth)\n",
        "\n",
        "        retrieval_metrics_log.append({\n",
        "            'query_id': query_id,\n",
        "            'retrieval_method': retrieval_method,\n",
        "            'prediction_algorithm_context': prediction_algorithm, # Context of which prediction this retrieval was for\n",
        "            f'precision_at_{K_EVAL}': precision,\n",
        "            f'recall_at_{K_EVAL}': recall,\n",
        "            f'f1_score_at_{K_EVAL}': f1\n",
        "        })\n",
        "\n",
        "    df_retrieval_metrics = pd.DataFrame(retrieval_metrics_log)\n",
        "\n",
        "    if not df_retrieval_metrics.empty:\n",
        "        # Save individual query metrics\n",
        "        filepath = os.path.join(PATH_EVAL_OUTPUT, \"retrieval_metrics_per_query.csv\")\n",
        "        df_retrieval_metrics.to_csv(filepath, index=False, encoding='utf-8')\n",
        "        print(f\"Per-query retrieval metrics saved to: {filepath}\")\n",
        "        display(df_retrieval_metrics.head())\n",
        "\n",
        "        # Calculate and display average metrics per retrieval_method\n",
        "        # We need to be careful if a query_id appears multiple times due to different prediction_algorithms\n",
        "        # For now, let's average based on unique (query_id, retrieval_method) pairs if possible,\n",
        "        # or just group by retrieval_method.\n",
        "        # Taking the first entry per query_id if retrieval_method is the same.\n",
        "        # Or, if predictions.csv has one line per (query_id, retrieval_method, prediction_algorithm)...\n",
        "\n",
        "        # Assuming one retrieval event per (query_id, retrieval_method) if prediction_algorithm varies.\n",
        "        # Let's average metrics by `retrieval_method`.\n",
        "        # If `predictions.csv` has one entry per (query, retrieval_method, prediction_algo), then\n",
        "        # the retrieval part is the same for entries with same query & retrieval_method.\n",
        "        # So, we can drop duplicates for averaging retrieval performance for that specific retrieval method.\n",
        "        cols_for_avg = [f'precision_at_{K_EVAL}', f'recall_at_{K_EVAL}', f'f1_score_at_{K_EVAL}']\n",
        "        # Group by unique retrieval events to avoid double counting due to prediction algorithm variations\n",
        "        unique_retrieval_events = df_retrieval_metrics.drop_duplicates(subset=['query_id', 'retrieval_method'])\n",
        "\n",
        "        df_avg_retrieval_metrics = unique_retrieval_events.groupby('retrieval_method')[cols_for_avg].mean().reset_index()\n",
        "\n",
        "        print(\"\\nAverage Retrieval Metrics per Method:\")\n",
        "        display(df_avg_retrieval_metrics)\n",
        "        filepath_avg = os.path.join(PATH_EVAL_OUTPUT, RETRIEVAL_METRICS_FILENAME) # Main summary file\n",
        "        df_avg_retrieval_metrics.to_csv(filepath_avg, index=False, encoding='utf-8')\n",
        "        print(f\"Average retrieval metrics saved to: {filepath_avg}\")\n",
        "        return df_avg_retrieval_metrics\n",
        "    else:\n",
        "        print(\"No retrieval metrics were calculated.\")\n",
        "        return pd.DataFrame()"
      ],
      "metadata": {
        "id": "KUENlx_deHPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Tahap 5.b: Evaluasi Prediksi (Solution Reuse) ---\n",
        "# This requires ground truth for the *solution itself*, not just relevant documents.\n",
        "# E.g., if predicting 'amar_putusan', we need the 'true' amar_putusan for each query.\n",
        "def evaluate_prediction_performance():\n",
        "    global df_queries, df_predictions, df_cases\n",
        "    print(\"\\n--- Evaluating Prediction (Solution Reuse) Performance ---\")\n",
        "    print(\"NOTE: This evaluation requires a 'ground_truth_solution' for each query.\")\n",
        "    print(\"      Please ensure your 'queries.json' (or a similar mapping) provides this.\")\n",
        "    print(\"      For example, add a field like 'expected_amar_category' to each query object.\")\n",
        "\n",
        "    # Try to merge predictions with ground truth queries, specifically looking for a 'ground_truth_solution' field\n",
        "    # The 'ground_truth_solution' field is NOT standard in our current 'queries.json'.\n",
        "    # This is a placeholder for how it *would* be done.\n",
        "    # User needs to augment queries.json or provide this mapping.\n",
        "\n",
        "    if 'ground_truth_solution' not in df_queries.columns:\n",
        "        print(\"\\n'ground_truth_solution' column not found in queries data (df_queries).\")\n",
        "        print(\"Prediction evaluation against ground truth solution categories cannot be performed automatically.\")\n",
        "        print(\"Consider adding a 'ground_truth_solution' or 'expected_amar_category' field to your queries.json,\")\n",
        "        print(\"or manually comparing the 'predicted_solution' in 'predictions.csv' with actuals for some cases.\")\n",
        "        return pd.DataFrame() # Return empty if no ground truth for solutions\n",
        "\n",
        "    # If 'ground_truth_solution' exists:\n",
        "    eval_pred_df = pd.merge(df_predictions, df_queries[['query_id', 'ground_truth_solution']], on='query_id', how='left')\n",
        "\n",
        "    if eval_pred_df['ground_truth_solution'].isnull().all():\n",
        "        print(\"All 'ground_truth_solution' values are null. Cannot perform prediction evaluation.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Filter out rows where ground truth solution is missing for fair evaluation\n",
        "    eval_pred_df_valid = eval_pred_df.dropna(subset=['ground_truth_solution', 'predicted_solution'])\n",
        "    if eval_pred_df_valid.empty:\n",
        "        print(\"No valid entries with both predicted and ground truth solutions.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    prediction_metrics_log = []\n",
        "\n",
        "    # Group by prediction_algorithm and retrieval_method to evaluate them separately\n",
        "    grouped = eval_pred_df_valid.groupby(['retrieval_method', 'prediction_algorithm'])\n",
        "    all_prediction_reports = {}\n",
        "\n",
        "    for name, group in grouped:\n",
        "        ret_method, pred_algo = name\n",
        "        y_true = group['ground_truth_solution']\n",
        "        y_pred = group['predicted_solution']\n",
        "\n",
        "        if len(y_true) == 0: continue # Skip if no data for this group\n",
        "\n",
        "        # Assuming solutions are categorical. sklearn.metrics are suitable.\n",
        "        accuracy = accuracy_score(y_true, y_pred)\n",
        "        # Precision, Recall, F1 per class (if multi-class) and average\n",
        "        # Use zero_division=0 to handle cases where a class might not be predicted or present in y_true for small samples\n",
        "        precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(y_true, y_pred, average='macro', zero_division=0)\n",
        "        precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted', zero_division=0)\n",
        "\n",
        "        report_str = classification_report(y_true, y_pred, zero_division=0)\n",
        "        all_prediction_reports[f\"{ret_method}_{pred_algo}\"] = report_str\n",
        "        print(f\"\\nClassification Report for {ret_method} (Retrieval) + {pred_algo} (Prediction):\")\n",
        "        print(report_str)\n",
        "\n",
        "        prediction_metrics_log.append({\n",
        "            'retrieval_method': ret_method,\n",
        "            'prediction_algorithm': pred_algo,\n",
        "            'accuracy': accuracy,\n",
        "            'precision_macro': precision_macro,\n",
        "            'recall_macro': recall_macro,\n",
        "            'f1_score_macro': f1_macro,\n",
        "            'precision_weighted': precision_weighted,\n",
        "            'recall_weighted': recall_weighted,\n",
        "            'f1_score_weighted': f1_weighted,\n",
        "        })\n",
        "\n",
        "    df_prediction_metrics = pd.DataFrame(prediction_metrics_log)\n",
        "\n",
        "    if not df_prediction_metrics.empty:\n",
        "        display(df_prediction_metrics)\n",
        "        filepath = os.path.join(PATH_EVAL_OUTPUT, PREDICTION_METRICS_FILENAME)\n",
        "        df_prediction_metrics.to_csv(filepath, index=False, encoding='utf-8')\n",
        "        print(f\"Prediction metrics saved to: {filepath}\")\n",
        "        # Save detailed classification reports too\n",
        "        with open(os.path.join(PATH_EVAL_OUTPUT, \"prediction_classification_reports.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
        "            for name, report in all_prediction_reports.items():\n",
        "                f.write(f\"--- Report for: {name} ---\\n\")\n",
        "                f.write(report)\n",
        "                f.write(\"\\n\\n\")\n",
        "        print(f\"Detailed classification reports saved to prediction_classification_reports.txt\")\n",
        "\n",
        "        return df_prediction_metrics\n",
        "    else:\n",
        "        print(\"No prediction metrics were calculated (likely due to missing ground_truth_solution).\")\n",
        "        return pd.DataFrame()"
      ],
      "metadata": {
        "id": "LL3xBKC0eJbY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Tahap 5.c: Visualisasi & Laporan (Analisis Kegagalan) ---\n",
        "def visualize_and_analyze(df_avg_retrieval_metrics, df_prediction_metrics):\n",
        "    print(\"\\n--- Visualizing Performance ---\")\n",
        "\n",
        "    # Plot Retrieval Metrics (Optional)\n",
        "    if df_avg_retrieval_metrics is not None and not df_avg_retrieval_metrics.empty:\n",
        "        df_avg_retrieval_metrics.set_index('retrieval_method', inplace=True)\n",
        "        df_avg_retrieval_metrics.plot(kind='bar', figsize=(12, 7), colormap='viridis')\n",
        "        plt.title(f'Average Retrieval Performance @ K={K_EVAL}')\n",
        "        plt.ylabel('Score')\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(PATH_EVAL_OUTPUT, 'retrieval_performance_chart.png'))\n",
        "        plt.show()\n",
        "        df_avg_retrieval_metrics.reset_index(inplace=True) # Reset index for consistency\n",
        "\n",
        "    # Plot Prediction Metrics (Optional)\n",
        "    if df_prediction_metrics is not None and not df_prediction_metrics.empty:\n",
        "        # Plotting accuracy and F1-macro for different prediction setups\n",
        "        plot_df = df_prediction_metrics.set_index(['retrieval_method', 'prediction_algorithm'])[['accuracy', 'f1_score_macro']]\n",
        "        plot_df.plot(kind='bar', figsize=(14, 8), colormap='coolwarm')\n",
        "        plt.title('Prediction Performance (Accuracy & F1-Macro)')\n",
        "        plt.ylabel('Score')\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(PATH_EVAL_OUTPUT, 'prediction_performance_chart.png'))\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "    print(\"\\n--- Analisis Kegagalan (Error Analysis / Rejection) ---\") #\n",
        "    print(\"This section requires manual inspection of cases where the model performed poorly.\")\n",
        "    print(\"Steps for Error Analysis:\")\n",
        "    print(\"1. Identify Queries with Poor Retrieval: Look at 'retrieval_metrics_per_query.csv'. Filter for queries with low F1-score.\")\n",
        "    print(\"   Example: `df_retrieval_metrics[df_retrieval_metrics[f'f1_score_at_{K_EVAL}'] < 0.3]`\")\n",
        "    print(\"2. For these queries:\")\n",
        "    print(\"   a. Examine the query text.\")\n",
        "    print(\"   b. Compare 'top_k_case_ids_list' (retrieved by your system) with 'ground_truth_ids'.\")\n",
        "    print(\"   c. Read the content (e.g., 'ringkasan_fakta', 'amar_putusan') of both retrieved and ground truth cases from 'cases_processed.csv'.\")\n",
        "    print(\"3. Hypothesize Reasons for Failure:\")\n",
        "    print(\"   - Query Ambiguity: Is the query too vague or too specific?\")\n",
        "    print(\"   - Vocabulary Mismatch (TF-IDF): Does the query use different terms than relevant documents?\")\n",
        "    print(\"   - Lack of Semantic Understanding (TF-IDF): Does TF-IDF fail to capture deeper meaning?\")\n",
        "    print(\"   - BERT Limitations: Is the query out-of-domain for BERT's training? Is context handled well?\")\n",
        "    print(\"   - Data Quality: Are there issues in the text of the documents (e.g. OCR errors, boilerplate)?\")\n",
        "    print(\"   - Ground Truth Issues: Are the 'ground_truth_ids' definitely correct and comprehensive?\")\n",
        "    print(\"4. Analyze Poor Predictions (if prediction evaluation was possible):\")\n",
        "    print(\"   - If `predicted_solution` is wrong, why? Was it due to poor retrieval (wrong similar cases found)?\")\n",
        "    print(\"   - Or was the prediction algorithm (majority_vote, weighted_similarity) flawed for that specific set of retrieved solutions?\")\n",
        "    print(\"5. Document Findings and Suggest Improvements:\")\n",
        "    print(\"   - E.g., improve text preprocessing, use query expansion, fine-tune BERT, add more features, improve solution aggregation logic.\") #\n",
        "    print(\"\\nExample: To get details for a poorly performing query 'QXXX':\")\n",
        "    # print(\"   `poor_query_details = df_predictions[df_predictions['query_id'] == 'QXXX']`\")\n",
        "    # print(\"   `display(poor_query_details)`\")\n",
        "    # print(\"   `ground_truth_for_poor_query = df_queries[df_queries['query_id'] == 'QXXX']['ground_truth_ids'].iloc[0]`\")\n",
        "    # print(\"   `print(f'Ground truth: {ground_truth_for_poor_query}')`\")\n",
        "    # print(\"   # Then look up these case_ids in df_cases\")"
      ],
      "metadata": {
        "id": "8H1PfYimeLLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Main Execution ---\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Starting Tahap 5: Model Evaluation\")\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "    if load_data_for_evaluation():\n",
        "        df_avg_ret_metrics = evaluate_retrieval_performance()\n",
        "        df_pred_metrics = evaluate_prediction_performance() # This might return empty if no ground_truth_solution\n",
        "        visualize_and_analyze(df_avg_ret_metrics, df_pred_metrics)\n",
        "    else:\n",
        "        print(\"Failed to load necessary data. Evaluation cannot proceed.\")\n",
        "\n",
        "    print(\"\\nTahap 5: Model Evaluation - Complete.\")"
      ],
      "metadata": {
        "id": "57DaeFkneMv7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}