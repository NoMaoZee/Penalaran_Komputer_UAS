{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# TAHAP 1\n",
        "## Penalaran Komputer UAS\n",
        "## **Anggota:**\n",
        "## Haidar Dimas Heryanto - 202210370311088\n",
        "## Zeedan Mustami Argani - 202210370311104"
      ],
      "metadata": {
        "id": "GZB9qBoWTq3E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install pandas requests beautifulsoup4 pdfminer.six lxml > /dev/null 2>&1"
      ],
      "metadata": {
        "id": "aICIafNyTyr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import io\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import urllib\n",
        "from concurrent.futures import ThreadPoolExecutor, wait\n",
        "from datetime import date\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from pdfminer import high_level # For PDF text extraction\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "bf0KqQfET32r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "sGZciQsjT5gq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configuration Section ---\n",
        "# !!! IMPORTANT: YOU NEED TO SET THESE VARIABLES !!!\n",
        "BASE_DRIVE_PATH = \"/content/drive/MyDrive/Penalaran Komputer UAS\" # Change to your project folder\n",
        "# Example: \"https://putusan3.mahkamahagung.go.id/search.html?q=narkotika&jenis_perkara=pidana_khusus\"\n",
        "# Go to Mahkamah Agung website, search for your chosen case type, and copy the URL of the search results page.\n",
        "# This URL should ideally allow pagination (e.g., by changing a 'page=' parameter).\n",
        "MA_SEARCH_RESULT_URL = \"https://putusan3.mahkamahagung.go.id/search.html?q=Perdaganganorang\" # e.g., \"https://putusan3.mahkamahagung.go.id/search.html?q=narkotika\"\n",
        "# A keyword to name output files, can be derived from your search or case type\n",
        "KEYWORD_FOR_FILENAMING = \"Perdagangan Orang\" # e.g., \"narkotika\" or \"wanprestasi\"\n",
        "MIN_DOCUMENTS_TO_SCRAPE = 100 # As per project requirement\n",
        "MAX_SCRAPING_WORKERS = 4 # For ThreadPoolExecutor\n",
        "\n",
        "# Define paths using BASE_DRIVE_PATH\n",
        "PATH_RAW_TEXT_OUTPUT = os.path.join(BASE_DRIVE_PATH, \"data/raw\")\n",
        "PATH_PDF_DOWNLOAD = os.path.join(BASE_DRIVE_PATH, \"PDFs_Putusan\")\n",
        "PATH_INITIAL_SCRAPER_CSV = os.path.join(BASE_DRIVE_PATH, \"Scraper_CSVs\")\n",
        "PATH_LOGS = os.path.join(BASE_DRIVE_PATH, \"logs\")\n",
        "\n",
        "# Create directories if they don't exist\n",
        "os.makedirs(PATH_RAW_TEXT_OUTPUT, exist_ok=True)\n",
        "os.makedirs(PATH_PDF_DOWNLOAD, exist_ok=True)\n",
        "os.makedirs(PATH_INITIAL_SCRAPER_CSV, exist_ok=True)\n",
        "os.makedirs(PATH_LOGS, exist_ok=True)\n",
        "\n",
        "# Optional: Cleaning log file\n",
        "CLEANING_LOG_FILE = os.path.join(PATH_LOGS, \"cleaning.log\")"
      ],
      "metadata": {
        "id": "ZpCTVY1nT7QF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def log_cleaning_action(message):\n",
        "    \"\"\"Appends a message to the cleaning log file.\"\"\"\n",
        "    with open(CLEANING_LOG_FILE, \"a\", encoding=\"utf-8\") as f:\n",
        "        f.write(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} - {message}\\n\")\n",
        "    print(message)\n",
        "\n",
        "# --- Scraper Code (Adapted from your scraping.py) ---\n",
        "\n",
        "def open_page_bs(link):\n",
        "    \"\"\"Opens a page and returns a BeautifulSoup object.\"\"\"\n",
        "    count = 0\n",
        "    while count < 3:\n",
        "        try:\n",
        "            response = requests.get(link, timeout=200)\n",
        "            response.raise_for_status() # Raise an exception for HTTP errors\n",
        "            return BeautifulSoup(response.text, \"lxml\")\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            log_cleaning_action(f\"Error opening page {link}: {e}. Retrying {count+1}/3\")\n",
        "            count += 1\n",
        "            time.sleep(5)\n",
        "    return None\n",
        "\n",
        "def get_detail_from_table(soup, keyword):\n",
        "    \"\"\"Extracts detail text from a table cell next to the keyword cell.\"\"\"\n",
        "    try:\n",
        "        text = (\n",
        "            soup.find(lambda tag: tag.name == \"td\" and keyword in tag.text)\n",
        "            .find_next_sibling(\"td\") # More robust than find_next()\n",
        "            .get_text(separator=\" \", strip=True)\n",
        "        )\n",
        "        return text\n",
        "    except AttributeError: # Handles cases where keyword or next cell is not found\n",
        "        return \"\"\n",
        "\n",
        "def download_pdf_from_url(pdf_url, download_path):\n",
        "    \"\"\"Downloads a PDF from a URL and saves it.\"\"\"\n",
        "    try:\n",
        "        response = urllib.request.urlopen(pdf_url, timeout=200)\n",
        "        # Sanitize filename\n",
        "        original_filename = response.info().get_filename()\n",
        "        if original_filename:\n",
        "            # Replace problematic characters for filenames\n",
        "            safe_filename = re.sub(r'[\\\\/*?:\"<>|]', \"_\", original_filename)\n",
        "        else:\n",
        "            # Create a filename if not provided (e.g., from the URL path)\n",
        "            safe_filename = pdf_url.split('/')[-1]\n",
        "            if not safe_filename.lower().endswith(\".pdf\"):\n",
        "                 safe_filename += \".pdf\"\n",
        "        safe_filename = safe_filename.replace(\"/\", \"_\").replace(\" \", \"_\")\n",
        "\n",
        "\n",
        "        file_content = response.read()\n",
        "        filepath = os.path.join(download_path, safe_filename)\n",
        "        with open(filepath, \"wb\") as out_file:\n",
        "            out_file.write(file_content)\n",
        "        log_cleaning_action(f\"Successfully downloaded PDF: {safe_filename} to {download_path}\")\n",
        "        return io.BytesIO(file_content), safe_filename, filepath\n",
        "    except Exception as e:\n",
        "        log_cleaning_action(f\"Failed to download PDF from {pdf_url}: {e}\")\n",
        "        return None, None, None\n",
        "\n",
        "def basic_text_cleaning_ma(text):\n",
        "    \"\"\"Basic text cleaning for Mahkamah Agung documents.\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    # Remove standard MA disclaimers (extend as needed)\n",
        "    text = text.replace(\"M a h ka m a h A g u n g R e p u blik In d o n esia\\n\", \"\")\n",
        "    text = text.replace(\"Disclaimer\\n\", \"\")\n",
        "    text = text.replace(\"Kepaniteraan Mahkamah Agung Republik Indonesia berusaha untuk selalu mencantumkan informasi paling kini dan akurat sebagai bentuk komitmen Mahkamah Agung untuk pelayanan publik, transparansi dan akuntabilitas\\n\",\"\")\n",
        "    text = text.replace(\"pelaksanaan fungsi peradilan. Namun dalam hal-hal tertentu masih dimungkinkan terjadi permasalahan teknis terkait dengan akurasi dan keterkinian informasi yang kami sajikan, hal mana akan terus kami perbaiki dari waktu kewaktu.\\n\",\"\")\n",
        "    text = text.replace(\"Dalam hal Anda menemukan inakurasi informasi yang termuat pada situs ini atau informasi yang seharusnya ada, namun belum tersedia, maka harap segera hubungi Kepaniteraan Mahkamah Agung RI melalui :\\n\",\"\")\n",
        "    text = text.replace(\"Email : kepaniteraan@mahkamahagung.go.id Telp : 021-384 3348 (ext.318)\\n\", \"\") # Corrected typo in original\n",
        "\n",
        "    # Further cleaning as per Tahap 1.b.ii\n",
        "    # Remove headers/footers - this is tricky and might need pattern-based removal.\n",
        "    # For now, we'll rely on pdfminer's extraction and the specific MA disclaimers.\n",
        "    # More sophisticated header/footer removal might involve regex based on common patterns\n",
        "    # (e.g., \"Putusan Nomor ... / Halaman ... dari ...\")\n",
        "\n",
        "    # Normalize whitespace (replace multiple spaces/newlines with a single one)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    # Lowercase (optional, consider if case is important for later stages)\n",
        "    # text = text.lower() # As per, making it optional here.\n",
        "    # Remove punctuation (optional, can be done here or in preprocessing for TF-IDF/BERT)\n",
        "    # text = re.sub(r'[^\\w\\s]', '', text) # Example: removes all except word chars and space\n",
        "\n",
        "    log_cleaning_action(\"Performed basic text cleaning (MA disclaimers, whitespace).\")\n",
        "    return text\n",
        "\n",
        "def convert_pdf_to_cleaned_text(pdf_content_stream):\n",
        "    \"\"\"Extracts text from PDF and applies cleaning.\"\"\"\n",
        "    try:\n",
        "        raw_text = high_level.extract_text(pdf_content_stream)\n",
        "        cleaned_text = basic_text_cleaning_ma(raw_text) # Apply MA specific cleaning\n",
        "        return cleaned_text\n",
        "    except Exception as e:\n",
        "        log_cleaning_action(f\"Error extracting text from PDF: {e}\")\n",
        "        return \"\""
      ],
      "metadata": {
        "id": "omlWkwgnT86v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Global list to store all extracted data for the final single CSV\n",
        "all_scraped_data = []\n",
        "# Global counter for raw text files\n",
        "raw_text_file_counter = 1\n",
        "# Lock for thread-safe operations on shared resources (if any become complex)\n",
        "# from threading import Lock\n",
        "# data_lock = Lock()"
      ],
      "metadata": {
        "id": "aJQg2Jy5T9gA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_decision_data(decision_page_url, unique_id_counter):\n",
        "    \"\"\"\n",
        "    Extracts metadata and text from a single court decision page.\n",
        "    Saves cleaned full text to a .txt file.\n",
        "    Returns a dictionary of extracted data.\n",
        "    \"\"\"\n",
        "    global raw_text_file_counter\n",
        "\n",
        "    log_cleaning_action(f\"Processing decision: {decision_page_url}\")\n",
        "    soup = open_page_bs(decision_page_url)\n",
        "    if not soup:\n",
        "        log_cleaning_action(f\"Failed to open page: {decision_page_url}. Skipping.\")\n",
        "        return None\n",
        "\n",
        "    # Attempt to find the main table containing decision details\n",
        "    table = soup.find(\"table\", {\"class\": \"table\"}) # Common class for such tables\n",
        "    if not table:\n",
        "        # Fallback: Try finding a table with 'Nomor Register' or 'Nomor Putusan'\n",
        "        table = soup.find(lambda tag: tag.name == \"table\" and (\"Nomor Register\" in tag.text or \"Nomor Putusan\" in tag.text))\n",
        "\n",
        "    if not table:\n",
        "        log_cleaning_action(f\"Could not find details table on {decision_page_url}. Skipping.\")\n",
        "        return None\n",
        "\n",
        "    # --- Metadata Extraction (similar to your script, using get_detail_from_table) ---\n",
        "    #\n",
        "    judul_putusan = table.find(\"h2\").text.strip() if table.find(\"h2\") else decision_page_url.split('/')[-2] # Use part of URL if no h2\n",
        "    if table.find(\"h2\"): table.find(\"h2\").decompose() # Remove h2 after extracting\n",
        "\n",
        "    nomor_perkara = get_detail_from_table(table, \"Nomor\") or get_detail_from_table(table, \"Nomor Register\") or get_detail_from_table(table, \"Nomor Putusan\")\n",
        "    tanggal_putusan = get_detail_from_table(table, \"Tanggal Dibacakan\") # Usually the decision date\n",
        "    if not tanggal_putusan: tanggal_putusan = get_detail_from_table(table, \"Tanggal Putusan\")\n",
        "    jenis_perkara = get_detail_from_table(table, \"Klasifikasi\") # 'Klasifikasi' often holds case type\n",
        "    if not jenis_perkara: jenis_perkara = get_detail_from_table(table, \"Jenis Perkara\")\n",
        "\n",
        "    # Extracting 'Pasal Digunakan' might require parsing the text body or specific sections\n",
        "    # For now, we'll leave a placeholder or try to find it in metadata if available\n",
        "    pasal_digunakan = get_detail_from_table(table, \"Kaidah\") # 'Kaidah' sometimes lists articles\n",
        "    if not pasal_digunakan : pasal_digunakan = get_detail_from_table(table,\"Pasal\") # If there's a direct 'Pasal' field\n",
        "\n",
        "    # Pihak can be complex (Penggugat/Tergugat, Terdakwa/Penuntut).\n",
        "    # This might need more advanced parsing if not explicitly listed.\n",
        "    # Simple approach:\n",
        "    pihak_1_label_td = table.find(\"td\", string=re.compile(r\"Pemohon|Penggugat|Penuntut\", re.IGNORECASE))\n",
        "    pihak_1 = pihak_1_label_td.find_next_sibling(\"td\").get_text(strip=True) if pihak_1_label_td else \"N/A\"\n",
        "\n",
        "    pihak_2_label_td = table.find(\"td\", string=re.compile(r\"Termohon|Tergugat|Terdakwa\", re.IGNORECASE))\n",
        "    pihak_2 = pihak_2_label_td.find_next_sibling(\"td\").get_text(strip=True) if pihak_2_label_td else \"N/A\"\n",
        "    nama_pihak = f\"{pihak_1} vs {pihak_2}\"\n",
        "\n",
        "\n",
        "    # Other metadata from your script (can be added if available and needed)\n",
        "    tingkat_proses = get_detail_from_table(table, \"Tingkat Proses\")\n",
        "    kata_kunci = get_detail_from_table(table, \"Kata Kunci\")\n",
        "    tahun_dokumen = get_detail_from_table(table, \"Tahun\")\n",
        "    tanggal_register = get_detail_from_table(table, \"Tanggal Register\")\n",
        "    lembaga_peradilan = get_detail_from_table(table, \"Lembaga Peradilan\")\n",
        "    amar_putusan = get_detail_from_table(table, \"Amar\") # Important for Tahap 4\n",
        "\n",
        "    # --- PDF Processing and Text Extraction ---\n",
        "    pdf_link_tag = soup.find(\"a\", href=re.compile(r\"/pdf/|/content/pdf/|/download/pdf\", re.IGNORECASE))\n",
        "    full_text_putusan = \"\"\n",
        "    pdf_filename_ondisk = \"\"\n",
        "    pdf_download_url = \"\"\n",
        "\n",
        "    if pdf_link_tag:\n",
        "        pdf_url_relative = pdf_link_tag[\"href\"]\n",
        "        # Construct absolute URL if relative\n",
        "        if pdf_url_relative.startswith(\"/\"):\n",
        "            base_ma_url = \"https://putusan3.mahkamahagung.go.id\" # Adjust if domain changes\n",
        "            pdf_download_url = base_ma_url + pdf_url_relative\n",
        "        else:\n",
        "            pdf_download_url = pdf_url_relative\n",
        "\n",
        "        log_cleaning_action(f\"Found PDF link: {pdf_download_url}\")\n",
        "        pdf_content_stream, pdf_filename_original, _ = download_pdf_from_url(pdf_download_url, PATH_PDF_DOWNLOAD)\n",
        "\n",
        "        if pdf_content_stream:\n",
        "            pdf_filename_ondisk = pdf_filename_original\n",
        "            extracted_text_from_pdf = convert_pdf_to_cleaned_text(pdf_content_stream) #\n",
        "            full_text_putusan = extracted_text_from_pdf\n",
        "\n",
        "            # Further cleaning (Tahap 1.a.ii, 1.b.ii)\n",
        "            # Normalisasi karakter (spasi sudah di basic_text_cleaning_ma, lowercase can be done later)\n",
        "            # Tokenisasi will be done in Tahap 2 or 3\n",
        "            # Hapus header/footer, nomor halaman, watermark (partially done in basic_text_cleaning_ma, advanced needs regex)\n",
        "            # Example for removing page numbers like \"Halaman X dari Y\" (very basic)\n",
        "            full_text_putusan = re.sub(r'Halaman\\s+\\d+\\s+dari\\s+\\d+', '', full_text_putusan, flags=re.IGNORECASE)\n",
        "            full_text_putusan = re.sub(r'Putusan Nomor:?\\s*[\\w\\/\\.\\-]+', '', full_text_putusan, flags=re.IGNORECASE) # Remove \"Putusan Nomor...\"\n",
        "            full_text_putusan = full_text_putusan.strip()\n",
        "\n",
        "\n",
        "            # Validate text integrity (Tahap 1.a.ii - Validasi)  (simple check)\n",
        "            if len(full_text_putusan) < 500: # Arbitrary threshold for minimal content\n",
        "                log_cleaning_action(f\"WARNING: Extracted text for {nomor_perkara} seems too short ({len(full_text_putusan)} chars). May not be complete.\")\n",
        "            else:\n",
        "                log_cleaning_action(f\"Extracted and cleaned text for {nomor_perkara}. Length: {len(full_text_putusan)} chars.\")\n",
        "\n",
        "            # Save cleaned full text to /data/raw/case_XXX.txt\n",
        "            # Ensure unique filename using a counter or the decision number if valid for filename\n",
        "            # Sanitize nomor_perkara for use as a filename\n",
        "            safe_nomor_perkara = re.sub(r'[\\\\/*?:\"<>|]', \"_\", nomor_perkara) if nomor_perkara else f\"case_{unique_id_counter:03d}\"\n",
        "            # Limit length and avoid issues\n",
        "            safe_nomor_perkara = (safe_nomor_perkara[:50] + '...') if len(safe_nomor_perkara) > 50 else safe_nomor_perkara\n",
        "\n",
        "            raw_text_filename = f\"case_{safe_nomor_perkara.replace('/', '_')}_{unique_id_counter:03d}.txt\"\n",
        "            raw_text_filepath = os.path.join(PATH_RAW_TEXT_OUTPUT, raw_text_filename)\n",
        "\n",
        "            try:\n",
        "                with open(raw_text_filepath, \"w\", encoding=\"utf-8\") as f:\n",
        "                    f.write(full_text_putusan)\n",
        "                log_cleaning_action(f\"Saved cleaned text to: {raw_text_filepath}\")\n",
        "            except Exception as e:\n",
        "                log_cleaning_action(f\"Error saving raw text file {raw_text_filepath}: {e}\")\n",
        "                # Fallback filename if nomor_perkara causes issues\n",
        "                raw_text_filename_fallback = f\"case_{unique_id_counter:03d}.txt\"\n",
        "                raw_text_filepath_fallback = os.path.join(PATH_RAW_TEXT_OUTPUT, raw_text_filename_fallback)\n",
        "                try:\n",
        "                    with open(raw_text_filepath_fallback, \"w\", encoding=\"utf-8\") as f:\n",
        "                        f.write(full_text_putusan)\n",
        "                    log_cleaning_action(f\"Saved cleaned text with fallback name: {raw_text_filepath_fallback}\")\n",
        "                except Exception as e_fallback:\n",
        "                    log_cleaning_action(f\"Critical error saving raw text file even with fallback name: {e_fallback}\")\n",
        "\n",
        "            # Increment counter for next file\n",
        "            # Handled by the loop that calls this function to ensure unique IDs even with threading issues\n",
        "\n",
        "    else:\n",
        "        log_cleaning_action(f\"No PDF link found on {decision_page_url}\")\n",
        "        full_text_putusan = \"NOT_AVAILABLE (NO_PDF_LINK)\"\n",
        "\n",
        "\n",
        "    # Prepare data for CSV\n",
        "    decision_data = {\n",
        "        \"case_id\": f\"case_{unique_id_counter:03d}\",\n",
        "        \"judul_putusan\": judul_putusan,\n",
        "        \"nomor_perkara\": nomor_perkara,\n",
        "        \"tanggal_putusan\": tanggal_putusan,\n",
        "        \"jenis_perkara\": jenis_perkara, # From 'Klasifikasi'\n",
        "        \"pasal_digunakan\": pasal_digunakan, # Placeholder, needs better extraction\n",
        "        \"nama_pihak\": nama_pihak, # Placeholder\n",
        "        \"tingkat_proses\": tingkat_proses,\n",
        "        \"kata_kunci\": kata_kunci,\n",
        "        \"tahun_dokumen\": tahun_dokumen,\n",
        "        \"tanggal_register\": tanggal_register,\n",
        "        \"lembaga_peradilan\": lembaga_peradilan,\n",
        "        \"amar_putusan\": amar_putusan,\n",
        "        \"link_sumber\": decision_page_url,\n",
        "        \"link_pdf\": pdf_download_url,\n",
        "        \"nama_file_pdf\": pdf_filename_ondisk,\n",
        "        \"nama_file_raw_text\": os.path.basename(raw_text_filepath if 'raw_text_filepath' in locals() and pdf_content_stream else \"N/A\"),\n",
        "        \"full_text_putusan_preview\": full_text_putusan[:200] + \"...\" # Preview for CSV\n",
        "        # Full text is in the .txt file. Avoid large text in CSV if possible.\n",
        "    }\n",
        "    return decision_data"
      ],
      "metadata": {
        "id": "AaIdLqvXUAvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_scraping_process(base_search_url, keyword_for_naming, num_documents_target):\n",
        "    \"\"\"\n",
        "    Manages the scraping of multiple pages and decisions.\n",
        "    \"\"\"\n",
        "    global all_scraped_data # Use the global list\n",
        "    global raw_text_file_counter # Use the global counter\n",
        "\n",
        "    if not base_search_url or base_search_url == \"YOUR_CHOSEN_SEARCH_URL\":\n",
        "        print(\"ERROR: MA_SEARCH_RESULT_URL is not set. Please provide a valid search URL.\")\n",
        "        return\n",
        "\n",
        "    log_cleaning_action(f\"Starting scraping process for keyword: {keyword_for_naming}\")\n",
        "    log_cleaning_action(f\"Target number of documents: {num_documents_target}\")\n",
        "\n",
        "    collected_decision_links = set()\n",
        "    current_page = 1\n",
        "    documents_collected_count = 0\n",
        "\n",
        "    # --- Part 1: Collect all unique decision links ---\n",
        "    while documents_collected_count < num_documents_target:\n",
        "        # Construct page URL (MA specific, may need adjustment based on actual URL structure)\n",
        "        # Common patterns: &page=NUMBER or ?page=NUMBER or /page/NUMBER\n",
        "        if \"&page=\" in base_search_url:\n",
        "            page_url = re.sub(r'&page=\\d+', f'&page={current_page}', base_search_url)\n",
        "        elif \"?page=\" in base_search_url:\n",
        "             page_url = re.sub(r'\\?page=\\d+', f'?page={current_page}', base_search_url)\n",
        "        elif not \"page=\" in base_search_url : # if no page parameter, try adding it\n",
        "            separator = '&' if '?' in base_search_url else '?'\n",
        "            page_url = f\"{base_search_url}{separator}page={current_page}\"\n",
        "        else: # Default if unsure\n",
        "            page_url = f\"{base_search_url}&page={current_page}\" # Assume this structure if others don't match\n",
        "\n",
        "        # If it's the first page and the base_search_url already implies page 1\n",
        "        if current_page == 1 and (\"&page=1\" in base_search_url or \"?page=1\" in base_search_url):\n",
        "            page_url = base_search_url\n",
        "\n",
        "\n",
        "        log_cleaning_action(f\"Accessing search results page: {page_url}\")\n",
        "        soup = open_page_bs(page_url)\n",
        "        if not soup:\n",
        "            log_cleaning_action(f\"Failed to load page {current_page}. Stopping link collection.\")\n",
        "            break\n",
        "\n",
        "        # Find links to individual decisions (adjust selector if needed)\n",
        "        # Links are usually within <a> tags, href containing \"/direktori/putusan/\"\n",
        "        decision_links_on_page = soup.find_all(\"a\", href=re.compile(r\"/direktori/putusan/.+\\.html\", re.IGNORECASE))\n",
        "\n",
        "        if not decision_links_on_page:\n",
        "            log_cleaning_action(f\"No more decision links found on page {current_page}. Total links so far: {len(collected_decision_links)}\")\n",
        "            break # No more links, assume end of results\n",
        "\n",
        "        new_links_found_on_page = 0\n",
        "        for link_tag in decision_links_on_page:\n",
        "            relative_url = link_tag[\"href\"]\n",
        "            # Construct absolute URL\n",
        "            if relative_url.startswith(\"/\"):\n",
        "                base_ma_url = \"https://putusan3.mahkamahagung.go.id\" # Adjust if domain changes\n",
        "                full_decision_url = base_ma_url + relative_url\n",
        "            else: # Should ideally always be relative from MA site\n",
        "                full_decision_url = relative_url\n",
        "\n",
        "            if full_decision_url not in collected_decision_links:\n",
        "                collected_decision_links.add(full_decision_url)\n",
        "                new_links_found_on_page +=1\n",
        "                # documents_collected_count will be based on successful processing later\n",
        "            if len(collected_decision_links) >= num_documents_target * 1.5 : # Collect a bit more to account for processing failures\n",
        "                 break\n",
        "        log_cleaning_action(f\"Page {current_page}: Found {len(decision_links_on_page)} potential links. Added {new_links_found_on_page} new unique links.\")\n",
        "\n",
        "        if len(collected_decision_links) >= num_documents_target * 1.5:\n",
        "            log_cleaning_action(f\"Collected enough links ({len(collected_decision_links)}). Moving to data extraction.\")\n",
        "            break\n",
        "\n",
        "        current_page += 1\n",
        "        if current_page > 50 : # Safety break for too many pages (e.g. if num_documents_target is very high)\n",
        "            log_cleaning_action(\"Reached page 50, stopping link collection phase.\")\n",
        "            break\n",
        "        time.sleep(2) # Be respectful to the server\n",
        "\n",
        "    log_cleaning_action(f\"Total unique decision links collected: {len(collected_decision_links)}\")\n",
        "    if not collected_decision_links:\n",
        "        log_cleaning_action(\"No decision links found. Please check MA_SEARCH_RESULT_URL and website structure.\")\n",
        "        return\n",
        "\n",
        "    # --- Part 2: Process collected links using ThreadPoolExecutor ---\n",
        "    # Ensure raw_text_file_counter is correctly managed if threading is complex\n",
        "    # For simplicity here, we'll pass a unique ID to extract_decision_data\n",
        "    # based on enumeration, assuming it's sufficient for unique file naming.\n",
        "    # A more robust approach might involve a thread-safe counter.\n",
        "\n",
        "    processed_results = []\n",
        "    links_to_process = list(collected_decision_links)[:num_documents_target] # Process up to target number\n",
        "\n",
        "    # Using ThreadPoolExecutor to speed up I/O bound tasks (downloading, page requests)\n",
        "    with ThreadPoolExecutor(max_workers=MAX_SCRAPING_WORKERS) as executor:\n",
        "        # Each future will execute extract_decision_data for a link\n",
        "        # We pass 'i+1' as a unique_id_counter for each task\n",
        "        futures = [executor.submit(extract_decision_data, link, i + 1) for i, link in enumerate(links_to_process)]\n",
        "\n",
        "        for future in futures: # Iterate through futures as they complete (or in order)\n",
        "            try:\n",
        "                result = future.result(timeout=300) # Timeout for each task\n",
        "                if result:\n",
        "                    processed_results.append(result)\n",
        "                    # documents_collected_count +=1 # Count successful extractions\n",
        "                    if result and result.get(\"nama_file_pdf\", \"N/A\") != \"N/A\":\n",
        "                      documents_collected_count += 1\n",
        "                    log_cleaning_action(f\"Successfully processed and got data for case_id: {result.get('case_id', 'N/A')}\")\n",
        "                else:\n",
        "                    log_cleaning_action(\"A task returned no result (None).\")\n",
        "            except Exception as e:\n",
        "                log_cleaning_action(f\"A scraping task failed: {e}\")\n",
        "\n",
        "    all_scraped_data.extend(processed_results)\n",
        "    log_cleaning_action(f\"Successfully processed {documents_collected_count} documents.\")\n",
        "\n",
        "\n",
        "    # --- Part 3: Save all collected data to a single CSV ---\n",
        "    if all_scraped_data:\n",
        "        df = pd.DataFrame(all_scraped_data)\n",
        "        # Define CSV filename\n",
        "        today_date = date.today().strftime(\"%Y-%m-%d\")\n",
        "        csv_filename = f\"putusan_ma_{keyword_for_naming.replace(' ', '_')}_{today_date}.csv\"\n",
        "        csv_filepath = os.path.join(PATH_INITIAL_SCRAPER_CSV, csv_filename)\n",
        "\n",
        "        df.to_csv(csv_filepath, index=False, encoding=\"utf-8\")\n",
        "        log_cleaning_action(f\"All scraped data saved to: {csv_filepath}\")\n",
        "        log_cleaning_action(f\"Total documents in CSV: {len(df)}\")\n",
        "        display(df.head())\n",
        "    else:\n",
        "        log_cleaning_action(\"No data was scraped successfully to save to CSV.\")\n",
        "\n",
        "    log_cleaning_action(\"Scraping and initial processing complete.\")\n",
        "    log_cleaning_action(f\"Cleaned text files saved in: {PATH_RAW_TEXT_OUTPUT}\")\n",
        "    log_cleaning_action(f\"PDFs downloaded to: {PATH_PDF_DOWNLOAD}\")"
      ],
      "metadata": {
        "id": "kIvGc0UHUEVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Main Execution ---\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Starting Tahap 1: Membangun Case Base (Scraping & Cleaning)\")\n",
        "\n",
        "    # Validate essential user-set variables\n",
        "    if MA_SEARCH_RESULT_URL == \"YOUR_CHOSEN_SEARCH_URL\" or not MA_SEARCH_RESULT_URL:\n",
        "        print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
        "        print(\"!!! ERROR: Please set 'MA_SEARCH_RESULT_URL' in the Configuration Section. !!!\")\n",
        "        print(\"!!! This should be the URL from the Mahkamah Agung website after searching for !!!\")\n",
        "        print(\"!!! your chosen 'jenis perkara'.                                               !!!\")\n",
        "        print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
        "    elif not KEYWORD_FOR_FILENAMING:\n",
        "        print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
        "        print(\"!!! ERROR: Please set 'KEYWORD_FOR_FILENAMING' in the Configuration Section.   !!!\")\n",
        "        print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
        "    else:\n",
        "        run_scraping_process(MA_SEARCH_RESULT_URL, KEYWORD_FOR_FILENAMING, MIN_DOCUMENTS_TO_SCRAPE)\n",
        "        print(f\"Scraping finished. Check the logs and output folders in: {BASE_DRIVE_PATH}\")\n",
        "        print(f\"Raw text files should be in: {PATH_RAW_TEXT_OUTPUT}\")\n",
        "        print(f\"Optional cleaning log: {CLEANING_LOG_FILE}\") #\n",
        "        print(f\"Output CSV from scraper: {PATH_INITIAL_SCRAPER_CSV}\")"
      ],
      "metadata": {
        "id": "FaQMN04DUE49"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}