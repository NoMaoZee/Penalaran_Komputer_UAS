{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# TAHAP 3\n",
        "## Penalaran Komputer UAS\n",
        "## **Anggota:**\n",
        "## Haidar Dimas Heryanto - 202210370311088\n",
        "## Zeedan Mustami Argani - 202210370311104"
      ],
      "metadata": {
        "id": "6PA_keZQTX6L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k3-BBDoCpyzl"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.model_selection import train_test_split # For splitting data\n",
        "import nltk\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install pandas requests beautifulsoup4 pdfminer.six lxml > /dev/null 2>&1\n",
        "\n",
        "import argparse\n",
        "import io\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import urllib\n",
        "from concurrent.futures import ThreadPoolExecutor, wait\n",
        "from datetime import date\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from pdfminer import high_level # For PDF text extraction\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- NLTK Setup (for word counting) ---\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "    # Add this line to also check and download 'punkt_tab'\n",
        "    nltk.data.find('tokenizers/punkt_tab')\n",
        "except LookupError:\n",
        "    print(\"NLTK 'punkt' or 'punkt_tab' not found. Downloading...\")\n",
        "    nltk.download('punkt')\n",
        "    nltk.download('punkt_tab') # Download the missing resource\n",
        "    print(\"NLTK 'punkt' and 'punkt_tab' downloaded.\")\n",
        "except Exception as e:\n",
        "     print(f\"An unexpected error occurred during NLTK setup: {e}\")"
      ],
      "metadata": {
        "id": "0s-E8J9fzWi6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For BERT (if chosen)\n",
        "# !pip install transformers sentence-transformers > /dev/null 2>&1 # sentence-transformers is often easier for embeddings\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "# from sentence_transformers import SentenceTransformer # Alternative for easier embeddings\n",
        "\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "DJ5pLPdUWPBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For BERT (if chosen)\n",
        "# !pip install transformers sentence-transformers > /dev/null 2>&1 # sentence-transformers is often easier for embeddings\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "# from sentence_transformers import SentenceTransformer # Alternative for easier embeddings\n",
        "\n",
        "from google.colab import drive\n",
        "import nltk # Import NLTK here as well if used in this cell\n",
        "import re # Import re if used in this cell\n",
        "\n",
        "# --- Configuration Section ---\n",
        "# !!! IMPORTANT: Ensure these paths match your Google Drive structure\n",
        "# and the outputs from Notebook 2 !!!\n",
        "BASE_DRIVE_PATH = \"/content/drive/MyDrive/Penalaran Komputer UAS/\" # Change to your project folder\n",
        "\n",
        "# Path for input processed data from Notebook 2\n",
        "PATH_PROCESSED_INPUT = os.path.join(BASE_DRIVE_PATH, \"data/processed\")\n",
        "PROCESSED_CSV_FILENAME = \"cases_processed.csv\" # Assuming this is the output from N2\n",
        "\n",
        "# Path for evaluation data output\n",
        "PATH_EVAL_OUTPUT = os.path.join(BASE_DRIVE_PATH, \"data/eval\")\n",
        "os.makedirs(PATH_EVAL_OUTPUT, exist_ok=True)\n",
        "QUERIES_JSON_FILENAME = \"queries.json\"\n",
        "\n",
        "# BERT Model (example)\n",
        "BERT_MODEL_NAME = 'indobenchmark/indobert-base-p1'\n",
        "# Or using SentenceTransformer: 'paraphrase-multilingual-MiniLM-L12-v2' or an Indonesian specific one if available\n",
        "\n",
        "# Determine device for PyTorch (BERT)\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "# --- NLTK Setup (for preprocessing if needed) ---\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "    # Catch LookupError specifically if a resource is not found\n",
        "    try:\n",
        "         nltk.data.find('corpora/stopwords')\n",
        "    except LookupError:\n",
        "         print(\"NLTK 'stopwords' not found. Downloading...\")\n",
        "         nltk.download('stopwords', quiet=True)\n",
        "         print(\"NLTK 'stopwords' downloaded.\")\n",
        "except LookupError:\n",
        "    # This block handles the case where 'punkt' is not found\n",
        "    print(\"NLTK 'punkt' not found. Downloading...\")\n",
        "    nltk.download('punkt', quiet=True)\n",
        "    print(\"NLTK 'punkt' downloaded.\")\n",
        "except Exception as e:\n",
        "     print(f\"An unexpected error occurred during NLTK setup: {e}\")\n",
        "\n",
        "# Now, safely load stopwords\n",
        "indonesian_stopwords = nltk.corpus.stopwords.words('indonesian')"
      ],
      "metadata": {
        "id": "UpYnuBpiWQrs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Helper Functions ---\n",
        "def preprocess_text_for_tfidf(text):\n",
        "    \"\"\"Basic preprocessing for TF-IDF: lowercase, remove punctuation, remove stopwords.\"\"\"\n",
        "    if pd.isna(text) or not text:\n",
        "        return \"\"\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text) # Remove punctuation\n",
        "    words = nltk.word_tokenize(text)\n",
        "    words = [word for word in words if word not in indonesian_stopwords and word.isalpha()]\n",
        "    return \" \".join(words)\n",
        "\n",
        "def load_processed_data(filepath):\n",
        "    \"\"\"Loads the processed data CSV.\"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(filepath)\n",
        "        print(f\"Successfully loaded processed data from: {filepath} with shape {df.shape}\")\n",
        "        # Ensure the text column to be used for retrieval is not all NaN\n",
        "        # Choose one: 'text_full', 'ringkasan_fakta', 'argumen_hukum_utama'\n",
        "        # For this example, let's use 'ringkasan_fakta' as it's more concise than 'text_full'\n",
        "        # and potentially more focused than 'argumen_hukum_utama' for general similarity.\n",
        "        # Or, you can combine them.\n",
        "        if 'ringkasan_fakta' in df.columns and df['ringkasan_fakta'].isna().all():\n",
        "            print(\"Warning: 'ringkasan_fakta' is all NaN. Falling back to 'text_full'.\")\n",
        "            df['retrieval_text_source'] = df['text_full']\n",
        "        elif 'ringkasan_fakta' in df.columns:\n",
        "            df['retrieval_text_source'] = df['ringkasan_fakta']\n",
        "        else:\n",
        "            print(\"Warning: 'ringkasan_fakta' not found. Using 'text_full'.\")\n",
        "            df['retrieval_text_source'] = df['text_full']\n",
        "\n",
        "        # Handle potential NaN values in the chosen source text by filling with empty string\n",
        "        df['retrieval_text_source'] = df['retrieval_text_source'].fillna('')\n",
        "\n",
        "        return df\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Processed data file not found at {filepath}. Please run Notebook 2 first.\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading processed data: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "o_GKigVaWS3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Tahap 3.a: Representasi Vektor ---\n",
        "\n",
        "# Option 1: TF-IDF\n",
        "print(\"\\n--- Initializing TF-IDF Components ---\")\n",
        "tfidf_vectorizer = TfidfVectorizer(preprocessor=preprocess_text_for_tfidf, max_df=0.95, min_df=2, ngram_range=(1,2))\n",
        "# max_df: ignore terms that appear in more than 95% of the documents\n",
        "# min_df: ignore terms that appear in less than 2 documents\n",
        "# ngram_range: consider unigrams and bigrams\n",
        "\n",
        "# Global variables to store fitted vectorizer and case vectors for TF-IDF\n",
        "fitted_tfidf_vectorizer = None\n",
        "case_vectors_tfidf = None\n",
        "case_ids_global_tfidf = None # To store the case_ids corresponding to case_vectors_tfidf"
      ],
      "metadata": {
        "id": "V_cB-EsPWU4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Option 2: BERT Embeddings\n",
        "print(\"\\n--- Initializing BERT Components ---\")\n",
        "try:\n",
        "    bert_tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL_NAME)\n",
        "    bert_model = AutoModel.from_pretrained(BERT_MODEL_NAME).to(DEVICE)\n",
        "    bert_model.eval() # Set model to evaluation mode\n",
        "    print(f\"BERT model '{BERT_MODEL_NAME}' loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Could not load BERT model '{BERT_MODEL_NAME}': {e}. BERT retrieval will not be available.\")\n",
        "    bert_tokenizer = None\n",
        "    bert_model = None\n",
        "\n",
        "# Global variables for BERT embeddings\n",
        "case_embeddings_bert = None # To store NumPy array of embeddings\n",
        "case_ids_global_bert = None # To store the case_ids corresponding to case_embeddings_bert\n",
        "BERT_EMBEDDING_DIM = 768 # For indobert-base models"
      ],
      "metadata": {
        "id": "pQZWAk1WWXAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_bert_embedding(text, tokenizer, model, device, max_length=512):\n",
        "    \"\"\"Generates embedding for a text using a pre-trained BERT model.\"\"\"\n",
        "    if not tokenizer or not model:\n",
        "        return np.zeros(BERT_EMBEDDING_DIM) # Return zero vector if model not loaded\n",
        "\n",
        "    # Preprocess: BERT has its own tokenizer, typically minimal cleaning like whitespace norm.\n",
        "    text = str(text).strip()\n",
        "    if not text: # Handle empty string\n",
        "        return np.zeros(BERT_EMBEDDING_DIM)\n",
        "\n",
        "    inputs = tokenizer(text, return_tensors='pt', max_length=max_length, truncation=True, padding='max_length')\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    # Use the embedding of the [CLS] token\n",
        "    cls_embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy().squeeze()\n",
        "    return cls_embedding\n",
        "\n",
        "def fit_vector_models(df_cases):\n",
        "    \"\"\"Fits TF-IDF and generates BERT embeddings for the case base.\"\"\"\n",
        "    global fitted_tfidf_vectorizer, case_vectors_tfidf, case_ids_global_tfidf\n",
        "    global case_embeddings_bert, case_ids_global_bert\n",
        "\n",
        "    if df_cases is None or df_cases.empty:\n",
        "        print(\"DataFrame is empty. Cannot fit vector models.\")\n",
        "        return\n",
        "\n",
        "    print(\"\\n[Fitting TF-IDF Vectorizer...]\")\n",
        "    # Use the 'retrieval_text_source' column prepared in load_processed_data\n",
        "    # Ensure there are no NaNs by filling with empty string (already done in load_processed_data)\n",
        "    texts_for_tfidf = df_cases['retrieval_text_source'].tolist()\n",
        "    try:\n",
        "        case_vectors_tfidf = tfidf_vectorizer.fit_transform(texts_for_tfidf)\n",
        "        fitted_tfidf_vectorizer = tfidf_vectorizer\n",
        "        case_ids_global_tfidf = df_cases['case_id'].tolist()\n",
        "        print(f\"TF-IDF fitting complete. Shape of case_vectors_tfidf: {case_vectors_tfidf.shape}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error fitting TF-IDF: {e}\")\n",
        "\n",
        "\n",
        "    print(\"\\n[Generating BERT Embeddings... (this may take a while)]\")\n",
        "    if bert_model and bert_tokenizer:\n",
        "        # Ensure 'retrieval_text_source' is used, and it's clean\n",
        "        texts_for_bert = df_cases['retrieval_text_source'].tolist()\n",
        "        embeddings_list = []\n",
        "        for i, text in enumerate(texts_for_bert):\n",
        "            # Give some progress feedback\n",
        "            if (i + 1) % 10 == 0 or i == len(texts_for_bert) - 1:\n",
        "                 print(f\"  Generating BERT embedding for document {i+1}/{len(texts_for_bert)}...\")\n",
        "            embedding = get_bert_embedding(text, bert_tokenizer, bert_model, DEVICE)\n",
        "            embeddings_list.append(embedding)\n",
        "\n",
        "        if embeddings_list:\n",
        "            case_embeddings_bert = np.array(embeddings_list)\n",
        "            case_ids_global_bert = df_cases['case_id'].tolist()\n",
        "            print(f\"BERT embeddings generation complete. Shape: {case_embeddings_bert.shape}\")\n",
        "        else:\n",
        "            print(\"No BERT embeddings were generated.\")\n",
        "    else:\n",
        "        print(\"BERT model not loaded. Skipping BERT embedding generation.\")"
      ],
      "metadata": {
        "id": "F1ZVUZRXWZ0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Tahap 3.c: Fungsi Retrieval ---\n",
        "# As per PDF: def retrieve(query: str, k: int = 5) -> List[case_id]\n",
        "\n",
        "def retrieve_cases(query_text, retrieval_method=\"tfidf\", k=5):\n",
        "    \"\"\"\n",
        "    Retrieves top-k similar case IDs for a given query text.\n",
        "    Methods: \"tfidf\" or \"bert\".\n",
        "    \"\"\"\n",
        "    if retrieval_method == \"tfidf\":\n",
        "        if fitted_tfidf_vectorizer is None or case_vectors_tfidf is None:\n",
        "            print(\"TF-IDF model not fitted. Please run `fit_vector_models` first.\")\n",
        "            return []\n",
        "        # Preprocess query same way as documents\n",
        "        processed_query = preprocess_text_for_tfidf(query_text)\n",
        "        query_vector = fitted_tfidf_vectorizer.transform([processed_query])\n",
        "        similarities = cosine_similarity(query_vector, case_vectors_tfidf).flatten()\n",
        "        # Get top-k indices\n",
        "        top_k_indices = similarities.argsort()[-k:][::-1]\n",
        "        top_k_case_ids = [case_ids_global_tfidf[i] for i in top_k_indices]\n",
        "        top_k_scores = [similarities[i] for i in top_k_indices]\n",
        "        print(f\"Retrieved using TF-IDF. Scores: {top_k_scores}\")\n",
        "        return top_k_case_ids\n",
        "\n",
        "    elif retrieval_method == \"bert\":\n",
        "        if case_embeddings_bert is None or not bert_model:\n",
        "            print(\"BERT embeddings not generated or model not loaded. Please run `fit_vector_models` or check BERT setup.\")\n",
        "            return []\n",
        "        query_embedding = get_bert_embedding(query_text, bert_tokenizer, bert_model, DEVICE)\n",
        "        query_embedding = query_embedding.reshape(1, -1) # Reshape for cosine_similarity\n",
        "        similarities = cosine_similarity(query_embedding, case_embeddings_bert).flatten()\n",
        "        top_k_indices = similarities.argsort()[-k:][::-1]\n",
        "        top_k_case_ids = [case_ids_global_bert[i] for i in top_k_indices]\n",
        "        top_k_scores = [similarities[i] for i in top_k_indices]\n",
        "        print(f\"Retrieved using BERT. Scores: {top_k_scores}\")\n",
        "        return top_k_case_ids\n",
        "    else:\n",
        "        print(f\"Unknown retrieval_method: {retrieval_method}. Choose 'tfidf' or 'bert'.\")\n",
        "        return []"
      ],
      "metadata": {
        "id": "J8am6VM2Wb5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Tahap 3.b: Splitting Data ---\n",
        "# The PDF mentions splitting data. For a pure retrieval system where the entire dataset forms the case base,\n",
        "# a formal train/test split of the *cases themselves* might not be for training the retrieval model (like TF-IDF or BERT embeddings)\n",
        "# but rather for evaluation purposes (e.g. if some cases are held out as queries).\n",
        "# Here, we consider the entire loaded dataset as our \"case base\".\n",
        "# The \"test queries\" will be defined separately in `queries.json`.\n",
        "\n",
        "# --- Tahap 3.d: Pengujian Awal & queries.json ---\n",
        "def create_and_save_sample_queries(filepath, df_cases_sample=None):\n",
        "    \"\"\"Creates sample queries and saves them to a JSON file.\"\"\"\n",
        "    # Create some sample queries. These should ideally be based on actual potential use cases\n",
        "    # or derived from a small subset of your data if you split it.\n",
        "    sample_queries_data = [\n",
        "        {\n",
        "            \"query_id\": \"Q001_PO\",\n",
        "            \"query_text\": \"Terdakwa merekrut korban dengan janji palsu pekerjaan sebagai pramusaji di kota besar, namun sesampainya di tujuan, korban justru disekap dan dipaksa menjadi pekerja seks komersial untuk keuntungan terdakwa.\",\n",
        "            \"ground_truth_ids\": [\"case_027\", \"case_046\", \"case_068\", \"case_086\", \"case_095\"], # Isi manual sesuai data Anda\n",
        "            \"ground_truth_solution\": \"Menolak Permohonan\" #Contoh\n",
        "        },\n",
        "        {\n",
        "            \"query_id\": \"Q002_PO\",\n",
        "            \"query_text\": \"Kasus melibatkan seorang asisten rumah tangga (ART) yang direkrut untuk bekerja di luar negeri. Pelaku menahan paspor dan alat komunikasi korban, memaksanya bekerja tanpa upah selama berbulan-bulan, dan melakukan kekerasan fisik jika korban menolak.\",\n",
        "            \"ground_truth_ids\": [\"case_078\", \"case_020\"], # Isi manual sesuai data Anda\n",
        "            \"ground_truth_solution\": \"Menolak Permohonan\"\n",
        "        },\n",
        "        {\n",
        "            \"query_id\": \"Q003_PO\",\n",
        "            \"query_text\": \"Pelaku memberikan pinjaman uang kepada keluarga korban dengan dalih untuk biaya pengobatan. Sebagai jaminan, anak korban dibawa untuk dipekerjakan. Namun, utang tersebut terus membengkak sehingga korban terjerat dalam situasi kerja paksa untuk melunasi utang.\",\n",
        "            \"ground_truth_ids\": [\"case_067\", \"case_047\"], # Isi manual sesuai data Anda\n",
        "            \"ground_truth_solution\": \"Menolak Permohonan\"\n",
        "        },\n",
        "        {\n",
        "            \"query_id\": \"Q004_PO\",\n",
        "            \"query_text\": \"Tindak pidana perdagangan anak di bawah umur yang dipekerjakan secara paksa di sebuah perkebunan. Para korban tidak diberi upah yang layak, dipaksa bekerja melebihi jam kerja normal, dan ditempatkan dalam kondisi hidup yang tidak manusiawi.\",\n",
        "            \"ground_truth_ids\": [\"case_049\", \"case_086\", \"case_027\", \"case_046\", \"case_095\", \"case_020\", \"case_078\", \"case_068\", \"case_072\"], # Isi manual sesuai data Anda\n",
        "            \"ground_truth_solution\":\"Menolak Permohonan\"\n",
        "        },\n",
        "        {\n",
        "            \"query_id\": \"Q005_PO\",\n",
        "            \"query_text\": \"Terdakwa menggunakan media sosial untuk memikat korban dengan menawarkan menjadi 'talent' atau model. Setelah korban bertemu, ia diancam dan dipaksa membuat konten pornografi yang hasilnya dijual oleh terdakwa secara online untuk keuntungan pribadi.\",\n",
        "            \"ground_truth_ids\": [\"case_049\", \"case_072\", \"case_078\"], # Isi manual sesuai data Anda\n",
        "            \"ground_truth_solution\": \"Menolak Permohonan\"\n",
        "        },\n",
        "        {\n",
        "            \"query_id\": \"Q006_PO\",\n",
        "            \"query_text\": \"Pelaku menggunakan Instagram dan TikTok untuk merekrut remaja perempuan dengan tawaran menjadi model atau influencer terkenal. Setelah bertemu, korban dibawa ke sebuah rumah, difoto dan direkam dalam kondisi tidak senonoh, kemudian dipaksa melayani klien dengan ancaman akan menyebar foto tersebut.\",\n",
        "            \"ground_truth_ids\": [\"case_096\", \"case_089\"], # Isi manual sesuai data Anda\n",
        "            \"ground_truth_solution\": \"Menolak Permohonan\"\n",
        "        },\n",
        "        {\n",
        "            \"query_id\": \"Q007_PO\",\n",
        "            \"query_text\": \"Sebuah sindikat perdagangan orang terorganisir yang melibatkan beberapa pelaku dengan peran berbeda: satu sebagai perekrut di desa, satu sebagai pengemudi, dan satu lagi sebagai penampung yang melakukan eksploitasi seksual terhadap korban.\",\n",
        "            \"ground_truth_ids\": [\"case_067\", \"case_066\", \"case_056\", \"case_051\", \"case_047\"], # Isi manual sesuai data Anda\n",
        "            \"ground_truth_solution\": \"Menolak Permohonan\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # Try to add some ground truth IDs if df_cases_sample is provided\n",
        "    # if df_cases_sample is not None and not df_cases_sample.empty and 'case_id' in df_cases_sample.columns:\n",
        "    #     all_case_ids = df_cases_sample['case_id'].tolist()\n",
        "    #     if len(all_case_ids) >= 2:\n",
        "    #         sample_queries_data[0][\"ground_truth_ids\"] = [all_case_ids[0]] # Example\n",
        "    #         if 'kasus narkotika' in df_cases_sample.iloc[0].get('retrieval_text_source','').lower() : # simple check\n",
        "    #              sample_queries_data[0][\"ground_truth_ids\"] = [df_cases_sample.iloc[0]['case_id']]\n",
        "\n",
        "    #     if len(all_case_ids) >= 5: # for Q002\n",
        "    #         # A more sophisticated way would be to find a case with \"ganja\"\n",
        "    #         for idx, row_q_sample in df_cases_sample.iterrows():\n",
        "    #             if \"ganja\" in str(row_q_sample.get('retrieval_text_source','')).lower():\n",
        "    #                 sample_queries_data[1][\"ground_truth_ids\"] = [row_q_sample['case_id']]\n",
        "    #                 break # found one\n",
        "    #         if not sample_queries_data[1][\"ground_truth_ids\"] and len(all_case_ids) > 1: # fallback\n",
        "    #              sample_queries_data[1][\"ground_truth_ids\"] = [all_case_ids[1]]\n",
        "\n",
        "\n",
        "    try:\n",
        "        with open(filepath, 'w', encoding='utf-8') as f:\n",
        "            json.dump(sample_queries_data, f, indent=4, ensure_ascii=False)\n",
        "        print(f\"Sample queries saved to: {filepath}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving sample queries: {e}\")\n",
        "    return sample_queries_data"
      ],
      "metadata": {
        "id": "Sruboh72WeRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Main Execution Logic ---\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Starting Tahap 3: Case Retrieval\")\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "    # 1. Load Data\n",
        "    processed_data_filepath = os.path.join(PATH_PROCESSED_INPUT, PROCESSED_CSV_FILENAME)\n",
        "    df_cases = load_processed_data(processed_data_filepath)\n",
        "\n",
        "    if df_cases is not None and not df_cases.empty:\n",
        "        # 2. Fit Vector Models (TF-IDF and/or BERT)\n",
        "        # This step prepares the case base for retrieval.\n",
        "        fit_vector_models(df_cases)\n",
        "\n",
        "        # 3. Create and Save Sample Queries for Initial Testing\n",
        "        queries_json_filepath = os.path.join(PATH_EVAL_OUTPUT, QUERIES_JSON_FILENAME)\n",
        "        # Pass a sample of df_cases to try to populate ground_truth_ids somewhat intelligently\n",
        "        sample_queries = create_and_save_sample_queries(queries_json_filepath, df_cases.head())\n",
        "\n",
        "\n",
        "        # 4. Test Retrieval Function\n",
        "        print(\"\\n--- Testing Retrieval Function ---\")\n",
        "        if sample_queries:\n",
        "            test_query_text = sample_queries[0]['query_text'] # Test with the first sample query\n",
        "            k_results = 3 # Retrieve top 3 for testing\n",
        "\n",
        "            print(f\"\\nQuery: '{test_query_text}'\")\n",
        "\n",
        "            # Test TF-IDF Retrieval\n",
        "            if fitted_tfidf_vectorizer:\n",
        "                print(f\"\\nRetrieving top {k_results} using TF-IDF...\")\n",
        "                retrieved_ids_tfidf = retrieve_cases(test_query_text, retrieval_method=\"tfidf\", k=k_results)\n",
        "                print(f\"TF-IDF Retrieved Case IDs: {retrieved_ids_tfidf}\")\n",
        "                # You can display details of retrieved cases:\n",
        "                if retrieved_ids_tfidf:\n",
        "                    display(df_cases[df_cases['case_id'].isin(retrieved_ids_tfidf)][['case_id', 'no_perkara', 'retrieval_text_source']].head())\n",
        "\n",
        "\n",
        "            # Test BERT Retrieval\n",
        "            if case_embeddings_bert is not None:\n",
        "                print(f\"\\nRetrieving top {k_results} using BERT...\")\n",
        "                retrieved_ids_bert = retrieve_cases(test_query_text, retrieval_method=\"bert\", k=k_results)\n",
        "                print(f\"BERT Retrieved Case IDs: {retrieved_ids_bert}\")\n",
        "                if retrieved_ids_bert:\n",
        "                     display(df_cases[df_cases['case_id'].isin(retrieved_ids_bert)][['case_id', 'no_perkara', 'retrieval_text_source']].head())\n",
        "\n",
        "        else:\n",
        "            print(\"No sample queries loaded to test retrieval.\")\n",
        "\n",
        "        # Note on SVM/Naive Bayes for \"classification/retrieval\"\n",
        "        print(\"\\n--- Note on SVM/Naive Bayes for Classification/Retrieval ---\")\n",
        "        print(\"The project mentions using SVM or Naive Bayes on TF-IDF for classification/retrieval.\")\n",
        "        print(\" - For CLASSIFICATION: If your cases have labels (e.g., 'outcome: guilty/not_guilty', 'type: Perdagangan Orang '),\")\n",
        "        print(\"   you could train SVM/Naive Bayes on the TF-IDF vectors to predict these labels for new cases.\")\n",
        "        print(\"   This requires a labeled dataset and splitting into train/test for the classifier.\")\n",
        "        print(\" - For RETRIEVAL using these classifiers: One approach could be to classify a query to a specific category,\")\n",
        "        print(\"   and then retrieve all cases from the case base belonging to that predicted category.\")\n",
        "        print(\"   This is different from direct similarity-based retrieval like cosine similarity.\")\n",
        "        print(\"   Implementation of this classification task is beyond the scope of this initial retrieval notebook but can be an extension.\")\n",
        "\n",
        "    else:\n",
        "        print(\"Failed to load data. Cannot proceed with retrieval.\")\n",
        "\n",
        "    print(\"\\nTahap 3: Case Retrieval - Complete.\")"
      ],
      "metadata": {
        "id": "mXpxFPcsWgON"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}