{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# TAHAP 2\n",
        "## Penalaran Komputer UAS\n",
        "## **Anggota:**\n",
        "## Haidar Dimas Heryanto - 202210370311088\n",
        "## Zeedan Mustami Argani - 202210370311104"
      ],
      "metadata": {
        "id": "_Q70cWDiTmrE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1IfE7az3p4aq"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "from google.colab import drive\n",
        "import nltk # Using NLTK for tokenization to count words"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configuration Section ---\n",
        "BASE_DRIVE_PATH = \"/content/drive/MyDrive/Penalaran Komputer UAS/\" # Change to your project folder\n",
        "\n",
        "# Paths for input data from Notebook 1\n",
        "PATH_RAW_TEXT_INPUT = os.path.join(BASE_DRIVE_PATH, \"data/raw\")\n",
        "PATH_INITIAL_SCRAPER_CSV_INPUT = os.path.join(BASE_DRIVE_PATH, \"Scraper_CSVs\")\n",
        "\n",
        "# Path for output processed data\n",
        "PATH_PROCESSED_OUTPUT = os.path.join(BASE_DRIVE_PATH, \"data/processed\")\n",
        "os.makedirs(PATH_PROCESSED_OUTPUT, exist_ok=True)\n",
        "\n",
        "# --- NLTK Setup (for word counting) ---\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "    # Add this line to also check and download 'punkt_tab'\n",
        "    nltk.data.find('tokenizers/punkt_tab')\n",
        "except LookupError:\n",
        "    print(\"NLTK 'punkt' or 'punkt_tab' not found. Downloading...\")\n",
        "    nltk.download('punkt')\n",
        "    nltk.download('punkt_tab') # Download the missing resource\n",
        "    print(\"NLTK 'punkt' and 'punkt_tab' downloaded.\")\n",
        "except Exception as e:\n",
        "     print(f\"An unexpected error occurred during NLTK setup: {e}\")"
      ],
      "metadata": {
        "id": "k5ewscU2UaTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Helper Functions ---\n",
        "\n",
        "def load_latest_scraper_csv(csv_folder_path):\n",
        "    \"\"\"Loads the most recently created CSV file from the specified folder.\"\"\"\n",
        "    try:\n",
        "        csv_files = [f for f in os.listdir(csv_folder_path) if f.endswith('.csv')]\n",
        "        if not csv_files:\n",
        "            print(f\"No CSV files found in {csv_folder_path}\")\n",
        "            return None\n",
        "        # Find the latest file based on filename pattern (if includes date) or modification time\n",
        "        # Assuming filenames might include dates like 'putusan_ma_KEYWORD_YYYY-MM-DD.csv'\n",
        "        csv_files.sort(key=lambda name: os.path.getmtime(os.path.join(csv_folder_path, name)), reverse=True)\n",
        "        latest_csv_filename = csv_files[0]\n",
        "        print(f\"Loading latest scraper CSV: {latest_csv_filename}\")\n",
        "        return pd.read_csv(os.path.join(csv_folder_path, latest_csv_filename))\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading latest CSV: {e}\")\n",
        "        return None\n",
        "\n",
        "def read_raw_text_file(filename, raw_text_folder):\n",
        "    \"\"\"Reads the content of a raw text file.\"\"\"\n",
        "    if not filename or filename.strip().upper() == \"A\":\n",
        "        print(f\"Skipping invalid filename: '{filename}'\")\n",
        "        return \"\"\n",
        "\n",
        "    filepath = os.path.join(raw_text_folder, filename)\n",
        "\n",
        "    if not os.path.exists(filepath):\n",
        "        print(f\"Warning: Raw text file does not exist: {filepath}\")\n",
        "        return \"\"\n",
        "\n",
        "    try:\n",
        "        with open(filepath, 'r', encoding='utf-8') as f:\n",
        "            return f.read()\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading raw text file {filepath}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def count_words(text):\n",
        "    \"\"\"Counts words in a text using NLTK tokenization.\"\"\"\n",
        "    if pd.isna(text) or not text:\n",
        "        return 0\n",
        "    tokens = nltk.word_tokenize(str(text))\n",
        "    return len(tokens)\n",
        "\n",
        "def extract_section_heuristic(text, keywords_start, keywords_end=None, limit_chars=5000, verbose=False):\n",
        "    \"\"\"\n",
        "    Extracts a section of text based on start and optional end keywords using heuristics.\n",
        "    - If start keyword found but end keyword not found: extract until limit_chars or end of text.\n",
        "    - If both found: extract between them.\n",
        "    - If none found: return \"\".\n",
        "\n",
        "    Parameters:\n",
        "        text (str): Full document text.\n",
        "        keywords_start (list): List of regex patterns (strings) for start.\n",
        "        keywords_end (list): List of regex patterns (strings) for end (optional).\n",
        "        limit_chars (int): Max character length of extracted segment.\n",
        "        verbose (bool): If True, print debug information.\n",
        "\n",
        "    Returns:\n",
        "        str: Extracted text section.\n",
        "    \"\"\"\n",
        "    if pd.isna(text) or not text:\n",
        "        return \"\"\n",
        "\n",
        "    text_lower = text.lower()\n",
        "    start_index = -1\n",
        "    matched_start_keyword = None\n",
        "\n",
        "    # 1. Cari start keyword\n",
        "    for kw_pattern in keywords_start:\n",
        "        match = re.search(kw_pattern, text_lower)\n",
        "        if match:\n",
        "            start_index = match.end()\n",
        "            matched_start_keyword = kw_pattern\n",
        "            break\n",
        "\n",
        "    if start_index == -1:\n",
        "        if verbose:\n",
        "            print(\"[extract_section_heuristic] Start keyword not found.\")\n",
        "        return \"\"\n",
        "\n",
        "    # 2. Cari end keyword (opsional)\n",
        "    end_index = len(text)\n",
        "    matched_end_keyword = None\n",
        "\n",
        "    if keywords_end:\n",
        "        found_end = False\n",
        "        for kw_pattern in keywords_end:\n",
        "            match = re.search(kw_pattern, text_lower[start_index:])\n",
        "            if match:\n",
        "                temp_end_index = start_index + match.start()\n",
        "                if temp_end_index < end_index:\n",
        "                    end_index = temp_end_index\n",
        "                    matched_end_keyword = kw_pattern\n",
        "                    found_end = True\n",
        "\n",
        "        if not found_end and verbose:\n",
        "            print(f\"[extract_section_heuristic] End keyword not found. Extracting until limit ({limit_chars}).\")\n",
        "\n",
        "    extracted = text[start_index:end_index].strip()\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"[extract_section_heuristic] Start match: {matched_start_keyword}\")\n",
        "        print(f\"[extract_section_heuristic] End match: {matched_end_keyword}\")\n",
        "        print(f\"[extract_section_heuristic] Extracted length: {len(extracted)} chars\")\n",
        "\n",
        "    return extracted[:limit_chars]"
      ],
      "metadata": {
        "id": "juwD7Ua8Ud37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Main Processing Logic ---\n",
        "print(\"Starting Tahap 2: Case Representation\")\n",
        "\n",
        "# 1. Load initial data (CSV from scraper and raw text files)\n",
        "print(f\"\\n[1. Loading Data]\")\n",
        "df_initial = load_latest_scraper_csv(PATH_INITIAL_SCRAPER_CSV_INPUT)\n",
        "\n",
        "# --- Filter rows: Remove rows with 'A' or NaN in 'nama_file_raw_text' ---\n",
        "if 'nama_file_raw_text' in df_initial.columns:\n",
        "    df_initial = df_initial[\n",
        "        df_initial['nama_file_raw_text'].notna() &  # Bukan NaN\n",
        "        ~df_initial['nama_file_raw_text'].str.contains(r'\\bA\\b', na=False)  # Tidak mengandung huruf A tunggal\n",
        "    ]\n",
        "    print(f\"Filtered data, remaining rows after removing invalid 'nama_file_raw_text': {len(df_initial)}\")\n",
        "\n",
        "if df_initial is None or df_initial.empty:\n",
        "    print(\"Could not load initial scraper CSV. Please ensure Notebook 1 ran successfully and the CSV exists.\")\n",
        "    # Exit or handle error appropriately\n",
        "else:\n",
        "    print(f\"Loaded initial data with {len(df_initial)} records.\")\n",
        "    # Ensure 'nama_file_raw_text' column exists\n",
        "    if 'nama_file_raw_text' not in df_initial.columns:\n",
        "        print(\"ERROR: 'nama_file_raw_text' column not found in the CSV. This column is needed to load full text.\")\n",
        "        # Exit or handle\n",
        "    else:\n",
        "        # Load full text for each case\n",
        "        df_initial['full_text_putusan'] = df_initial['nama_file_raw_text'].apply(\n",
        "            lambda x: read_raw_text_file(x, PATH_RAW_TEXT_INPUT) if pd.notna(x) else \"\"\n",
        "        )\n",
        "        print(\"Full text loaded into DataFrame.\")\n",
        "\n",
        "        # Initialize new columns for extracted features\n",
        "        df_initial['ringkasan_fakta'] = \"\"\n",
        "        df_initial['argumen_hukum_utama'] = \"\"\n",
        "        df_initial['pasal_digunakan_extracted'] = \"\" # For refined pasal extraction\n",
        "        df_initial['pihak_terlibat_extracted'] = \"\"   # For refined pihak extraction\n",
        "\n",
        "        # 2. Metadata Extraction & Refinement\n",
        "        # Most metadata (nomor perkara, tanggal, jenis perkara) is already in df_initial from the scraper.\n",
        "        # We might want to refine 'pasal_digunakan' and 'nama_pihak' if the initial scrape was basic.\n",
        "        print(f\"\\n[2. Extracting/Refining Metadata & Text Features]\")\n",
        "\n",
        "        # Keywords for extracting \"Ringkasan Fakta\"\n",
        "        # These are examples and might need adjustment based on actual document structures\n",
        "        fakta_keywords_start = [\n",
        "            r\"tentang pokok sengketa pengajuan peninjauan kembali\",\n",
        "            r\"alasan dan penjelasan permohonan banding\",\n",
        "            r\"alasan permohonan banding\",\n",
        "            r\"pokok sengketa;\",\n",
        "            r\"kronologis sengketa pajak\",\n",
        "            r\"menimbang,\\s*bahwa\\s*terdakwa\\s*diajukan\\s*ke\\s*persidangan\",\n",
        "            r\"menimbang,\\s*bahwa\\s*penggugat\\s*dalam\\s*surat\\s*gugatannya\",\n",
        "            r\"duduk\\s*perkara:\",\n",
        "            r\"fakta-fakta\\s*hukum\\s*yang\\s*terungkap\",\n",
        "            r\"menimbang,\\s*bahwa\\s*untuk\\s*membuktikan\\s*dalil-dalilnya\",\n",
        "            r\"uraian\\s*singkat\\s*mengenai\\s*kejadian\",\n",
        "            r\"tentang\\s*duduk\\s*perkara\"\n",
        "        ]\n",
        "        fakta_keywords_end = [ # Stop before legal considerations or verdict\n",
        "            r\"pertimbangan hukum\",\n",
        "            r\"tentang pertimbangan hukum\",\n",
        "            r\"menimbang, bahwa terhadap alasan-alasan peninjauan kembali\",\n",
        "            r\"menimbang,\\s*bahwa\\s*selanjutnya\\s*majelis\\s*hakim\\s*akan\\s*mempertimbangkan\",\n",
        "            r\"pertimbangan\\s*hukum\",\n",
        "            r\"tentang\\s*pertimbangan\\s*hukum\",\n",
        "            r\"amar\\s*putusan\",\n",
        "            r\"mengadili\",\n",
        "            r\"permohonan kasasi\", r\"duduk\\s*sengketa\", r\"pertimbangan\\s*permohonan\",\n",
        "            r\"permohonan pemohon\", r\"kejadian perkara\", r\"uraian kejadian\"\n",
        "        ]\n",
        "\n",
        "        # Keywords for \"Argumen Hukum Utama\" (Pertimbangan Hukum)\n",
        "        argumen_keywords_start = [\n",
        "            r\"pertimbangan hukum\", # Header utama\n",
        "            r\"menimbang, bahwa terhadap alasan-alasan peninjauan kembali tersebut, mahkamah agung berpendapat\",\n",
        "            r\"menimbang, bahwa alasan-alasan permohonan pemohon peninjauan kembali tidak dapat dibenarkan\",\n",
        "            r\"menimbang, bahwa alasan-alasan permohonan pemohon peninjauan kembali dapat dibenarkan\",\n",
        "            r\"pertimbangan\\s*hukum\",\n",
        "            r\"tentang\\s*pertimbangan\\s*hukum\",\n",
        "            r\"menimbang,\\s*bahwa\\s*terhadap\\s*eksepsi\", # Start of legal reasoning\n",
        "            r\"menimbang,\\s*bahwa\\s*majelis\\s*hakim\\s*berpendapat\",\n",
        "            r\"menimbang,\\s*bahwa\\s*oleh\\s*karena\\s*itu\\s*dengan\\s*memperhatikan\"\n",
        "        ]\n",
        "        argumen_keywords_end = [ # Stop before the final verdict/amar\n",
        "            r\"memperhatikan pasal-pasal dari undang-undang\",\n",
        "            r\"mengadili,\",\n",
        "            r\"amar\\s*putusan\",\n",
        "            r\"mengadili\",\n",
        "            r\"memutuskan\",\n",
        "            r\"menetapkan\"\n",
        "        ]\n",
        "\n",
        "        # Regex for \"Pasal Digunakan\" (example, very basic, often complex)\n",
        "        # Looks for patterns like \"Pasal X ayat (Y) UU No. Z Tahun A\" or KUHP/KUHAP etc.\n",
        "        pasal_regex_patterns = [\n",
        "            r\"pasal\\s*\\d+\\s*(ayat\\s*\\(?\\s*\\d+\\s*\\)?\\s*)?(huruf\\s*[a-z]\\s*)?\\s*(uu|undang-undang)\\s*(nomor|no\\.)?\\s*\\d+\\s*tahun\\s*\\d+\",\n",
        "            r\"pasal\\s*\\d+\\s*(ayat\\s*\\(?\\s*\\d+\\s*\\)?\\s*)?\\s*kuhp(?:idana)?\",\n",
        "            r\"pasal\\s*\\d+\\s*(ayat\\s*\\(?\\s*\\d+\\s*\\)?\\s*)?\\s*kuhperdata\",\n",
        "            r\"peraturan pemerintah\\s*(nomor|no\\.)?\\s*\\d+\\s*tahun\\s*\\d+\",\n",
        "            r\"peraturan menteri keuangan\\s*(nomor|no\\.)?[\\s\\w./-]+\", # Mencakup format seperti 78/PMK.03/2010\n",
        "            r\"keputusan direktur jenderal pajak\\s*(nomor|no\\.)?[\\s\\w./-]+\", # Mencakup format seperti KEP-539/PJ./2001\n",
        "            r\"surat edaran\\s*(direktur jenderal pajak)?\\s*(nomor|no\\.)?[\\s\\w./-]+\" # Mencakup format seperti SE-90/PJ/2011\n",
        "        ]\n",
        "\n",
        "        # Function to extract ringkasan fakta with fallback mechanism\n",
        "        def extract_ringkasan_fakta_enhanced(full_text, raw_filename, case_id):\n",
        "            \"\"\"\n",
        "            Enhanced function to extract ringkasan fakta with multiple fallback levels:\n",
        "            1. Try to extract from full_text (main putusan)\n",
        "            2. If not found, try to extract from raw text file directly\n",
        "            3. If still not found, return \"TIDAK TERDETEKSI\"\n",
        "            \"\"\"\n",
        "            print(f\"  -> Extracting ringkasan fakta for case_id: {case_id}\")\n",
        "\n",
        "            # Level 1: Try extracting from main full_text\n",
        "            ringkasan_level1 = extract_section_heuristic(\n",
        "                full_text, fakta_keywords_start, fakta_keywords_end, limit_chars=4000\n",
        "            )\n",
        "\n",
        "            if ringkasan_level1 and ringkasan_level1.strip() and len(ringkasan_level1.strip()) > 50:\n",
        "                print(f\"  -> Ringkasan fakta found in main text (Level 1)\")\n",
        "                return ringkasan_level1\n",
        "\n",
        "            # Level 2: Try reading raw file directly and extract from there\n",
        "            print(f\"  -> Ringkasan fakta not found in main text, trying raw file (Level 2)...\")\n",
        "            try:\n",
        "                if pd.notna(raw_filename) and raw_filename.strip():\n",
        "                    raw_text_direct = read_raw_text_file(raw_filename, PATH_RAW_TEXT_INPUT)\n",
        "                    if raw_text_direct and raw_text_direct.strip():\n",
        "                        ringkasan_level2 = extract_section_heuristic(\n",
        "                            raw_text_direct, fakta_keywords_start, fakta_keywords_end, limit_chars=4000\n",
        "                        )\n",
        "\n",
        "                        if ringkasan_level2 and ringkasan_level2.strip() and len(ringkasan_level2.strip()) > 50:\n",
        "                            print(f\"  -> Ringkasan fakta found in raw file (Level 2)\")\n",
        "                            return ringkasan_level2\n",
        "                        else:\n",
        "                            print(f\"  -> Ringkasan fakta not found in raw file either\")\n",
        "                    else:\n",
        "                        print(f\"  -> Raw file is empty or could not be read\")\n",
        "                else:\n",
        "                    print(f\"  -> Raw filename is invalid\")\n",
        "            except Exception as e:\n",
        "                print(f\"  -> Error reading raw file: {str(e)}\")\n",
        "\n",
        "            # Level 3: Not found anywhere\n",
        "            print(f\"  -> Ringkasan fakta TIDAK TERDETEKSI after all attempts\")\n",
        "            return \"TIDAK TERDETEKSI\"\n",
        "\n",
        "        for index, row in df_initial.iterrows():\n",
        "            full_text = str(row['full_text_putusan'])\n",
        "            full_text_lower = full_text.lower() # Gunakan versi lowercase untuk pencarian\n",
        "            raw_filename = row.get('nama_file_raw_text', '')\n",
        "            case_id = row.get('case_id', f'Index-{index}')\n",
        "\n",
        "            print(f\"Processing case_id: {case_id}...\")\n",
        "\n",
        "            # Ekstraksi Pihak Terlibat (Lebih Akurat)\n",
        "            # Pola: PEMOHON... melawan: TERMOHON...\n",
        "            pihak_match = re.search(\n",
        "                r\"(pemohon peninjauan kembali.*?)(?:melawan:|lawan)(.*?)(?:mahkamah agung tersebut;)\",\n",
        "                full_text,\n",
        "                re.IGNORECASE | re.DOTALL\n",
        "            )\n",
        "            if pihak_match:\n",
        "                pemohon_text = re.sub(r'\\s+', ' ', pihak_match.group(1)).strip()\n",
        "                termohon_text = re.sub(r'\\s+', ' ', pihak_match.group(2)).strip()\n",
        "                df_initial.loc[index, 'pihak_terlibat_extracted'] = f\"Pemohon: {pemohon_text} vs Termohon: {termohon_text}\"\n",
        "            else:\n",
        "                # Fallback jika pola utama tidak ditemukan\n",
        "                df_initial.loc[index, 'pihak_terlibat_extracted'] = row.get('nama_pihak', 'N/A')\n",
        "\n",
        "            # Extract Ringkasan Fakta (menggunakan enhanced function dengan fallback)\n",
        "            df_initial.loc[index, 'ringkasan_fakta'] = extract_ringkasan_fakta_enhanced(\n",
        "                full_text, raw_filename, case_id\n",
        "            )\n",
        "\n",
        "            # Extract Argumen Hukum Utama (menggunakan keywords baru)\n",
        "            df_initial.loc[index, 'argumen_hukum_utama'] = extract_section_heuristic(\n",
        "                full_text, argumen_keywords_start, argumen_keywords_end, limit_chars=5000\n",
        "            ) or \"TIDAK TERDETEKSI\"\n",
        "\n",
        "            # Extract Pasal Digunakan (menggunakan regex baru)\n",
        "            found_pasal_list = []\n",
        "            # Cari di seluruh teks (versi lowercase)\n",
        "            for pattern in pasal_regex_patterns:\n",
        "                matches = re.findall(pattern, full_text_lower)\n",
        "                for match in matches:\n",
        "                    pasal_text = match if isinstance(match, str) else \" \".join(filter(None, match))\n",
        "                    normalized_pasal = ' '.join(pasal_text.split()).strip()\n",
        "                    if normalized_pasal and normalized_pasal not in found_pasal_list:\n",
        "                        found_pasal_list.append(normalized_pasal)\n",
        "\n",
        "            # Gabungkan hasil dan bersihkan dari duplikat\n",
        "            df_initial.loc[index, 'pasal_digunakan_extracted'] = \"; \".join(sorted(list(set(found_pasal_list)))) if found_pasal_list else row.get('pasal_digunakan', '')\n",
        "\n",
        "        # 3. Feature Engineering\n",
        "        print(f\"\\n[3. Performing Feature Engineering]\")\n",
        "\n",
        "        # Calculate length (jumlah kata) for key text fields\n",
        "        df_initial['jumlah_kata_full_text'] = df_initial['full_text_putusan'].apply(count_words)\n",
        "        df_initial['jumlah_kata_ringkasan_fakta'] = df_initial['ringkasan_fakta'].apply(count_words)\n",
        "        df_initial['jumlah_kata_argumen_hukum'] = df_initial['argumen_hukum_utama'].apply(count_words)\n",
        "        print(\"Word counts calculated.\")\n",
        "\n",
        "        # Bag-of-Words (BoW) - Will be implicitly handled by TF-IDF in Tahap 3.\n",
        "        # For this stage, we can note its conceptual presence or skip explicit generation\n",
        "        # to avoid large sparse matrices in this intermediate CSV.\n",
        "        # If needed, one could tokenize and store counts, but it's often not stored directly.\n",
        "        print(\"Conceptual Bag-of-Words representation will be handled in later stages (e.g., TF-IDF).\")\n",
        "\n",
        "        # QA-pairs sederhana - This is an advanced feature.\n",
        "        # For a \"sederhana\" system, this could be:\n",
        "        # - Placeholder: Indicating it's a potential future enhancement.\n",
        "        # - Heuristic: Extracting questions from \"Pertimbangan Hukum\" if any explicit questions are posed.\n",
        "        # For now, we'll add a placeholder column.\n",
        "        df_initial['qa_pairs_sederhana'] = \"NOT_IMPLEMENTED\" # Placeholder\n",
        "        print(\"QA-pairs (sederhana) marked as NOT_IMPLEMENTED (advanced feature).\")\n",
        "\n",
        "        # 4. Prepare Final DataFrame and Save\n",
        "        print(f\"\\n[4. Preparing and Saving Processed Data]\")\n",
        "\n",
        "        # Select and rename columns to match PDF example where possible\n",
        "        # \"case_id\", \"no_perkara\", \"tanggal\", \"ringkasan_fakta\", \"pasal\", \"pihak\", \"text_full\"\n",
        "        df_processed = df_initial.rename(columns={\n",
        "            'nomor_perkara': 'no_perkara',\n",
        "            'tanggal_putusan': 'tanggal', # Assuming tanggal_putusan is the main date\n",
        "            'pasal_digunakan_extracted': 'pasal', # Using the extracted/refined one\n",
        "            'pihak_terlibat_extracted': 'pihak',   # Using the extracted/refined one\n",
        "            'full_text_putusan': 'text_full' # Full text is important\n",
        "        })\n",
        "\n",
        "        # Ensure all required columns from PDF example are present, add if missing\n",
        "        required_cols = [\"case_id\", \"no_perkara\", \"tanggal\", \"ringkasan_fakta\", \"pasal\", \"pihak\", \"text_full\"]\n",
        "        for col in required_cols:\n",
        "            if col not in df_processed.columns:\n",
        "                df_processed[col] = df_initial.get(col, pd.NA) # Get from original if renamed, else NA\n",
        "\n",
        "        # Add other valuable columns (metadata from scraper, engineered features)\n",
        "        # Keep original 'jenis_perkara', 'amar_putusan', etc.\n",
        "        # Keep word counts\n",
        "        additional_cols_to_keep = [\n",
        "            'judul_putusan', 'jenis_perkara', 'tingkat_proses', 'kata_kunci',\n",
        "            'tahun_dokumen', 'tanggal_register', 'lembaga_peradilan', 'amar_putusan',\n",
        "            'link_sumber', 'link_pdf', 'nama_file_pdf', 'nama_file_raw_text',\n",
        "            'jumlah_kata_full_text', 'jumlah_kata_ringkasan_fakta', 'jumlah_kata_argumen_hukum',\n",
        "            'argumen_hukum_utama', # Retain this important feature\n",
        "            'qa_pairs_sederhana'\n",
        "        ]\n",
        "\n",
        "        final_columns_ordered = required_cols + [col for col in additional_cols_to_keep if col in df_processed.columns and col not in required_cols]\n",
        "        # Ensure no duplicate columns and all are present\n",
        "        final_columns_ordered = sorted(list(set(final_columns_ordered)), key=final_columns_ordered.index)\n",
        "        df_processed = df_processed[final_columns_ordered]\n",
        "\n",
        "        # Save to CSV\n",
        "        processed_csv_filename = \"cases_processed.csv\"\n",
        "        processed_csv_filepath = os.path.join(PATH_PROCESSED_OUTPUT, processed_csv_filename)\n",
        "        df_processed.to_csv(processed_csv_filepath, index=False, encoding='utf-8')\n",
        "        print(f\"Processed data saved to: {processed_csv_filepath}\")\n",
        "\n",
        "        # Save to JSON (optional, as per PDF )\n",
        "        processed_json_filename = \"cases_processed.json\"\n",
        "        processed_json_filepath = os.path.join(PATH_PROCESSED_OUTPUT, processed_json_filename)\n",
        "        df_processed.to_json(processed_json_filepath, orient='records', indent=4, force_ascii=False)\n",
        "        print(f\"Processed data also saved to: {processed_json_filepath}\")\n",
        "\n",
        "        print(\"\\n--- Sample of Processed Data ---\")\n",
        "        display(df_processed.head())\n",
        "        print(f\"\\nColumns in processed DataFrame: {df_processed.columns.tolist()}\")\n",
        "        print(f\"Shape of processed DataFrame: {df_processed.shape}\")\n",
        "\n",
        "        # Summary statistics untuk ringkasan fakta\n",
        "        print(\"\\n--- Ringkasan Fakta Extraction Summary ---\")\n",
        "        ringkasan_status = df_processed['ringkasan_fakta'].apply(\n",
        "            lambda x: 'TERDETEKSI' if x != 'TIDAK TERDETEKSI' else 'TIDAK TERDETEKSI'\n",
        "        ).value_counts()\n",
        "        print(f\"Ringkasan Fakta Status:\\n{ringkasan_status}\")\n",
        "\n",
        "        # Show average length of detected ringkasan fakta\n",
        "        detected_ringkasan = df_processed[df_processed['ringkasan_fakta'] != 'TIDAK TERDETEKSI']['ringkasan_fakta']\n",
        "        if not detected_ringkasan.empty:\n",
        "            avg_length = detected_ringkasan.apply(len).mean()\n",
        "            print(f\"Average length of detected ringkasan fakta: {avg_length:.0f} characters\")\n",
        "\n",
        "print(\"\\nTahap 2: Case Representation - Complete.\")"
      ],
      "metadata": {
        "id": "5aobjKtSUgJ6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}